# Hybrid CDiT Memory System Design

## Core Design Philosophy

This project implements an intelligent Hybrid CDiT model that combines CDiT's precise conditional control with WorldMem's long-term memory capabilities. The core innovation lies in **separated storage and retrieval mechanisms**, enabling smarter memory management.

## Architecture Dependencies

### Core Component Dependencies from models.py

`hybrid_models.py` depends on and reuses the following core components from `models.py`:

```python
from models import TimestepEmbedder, ActionEmbedder, modulate, FinalLayer
```

**Component Functionality:**
- **`TimestepEmbedder`**: Uses sinusoidal-cosine embedding to convert scalar timesteps to vector representations
- **`ActionEmbedder`**: Embeds action vectors [x, y, yaw] into hidden representation space
- **`modulate`**: AdaLN (Adaptive Layer Normalization) modulation function for conditional control
- **`FinalLayer`**: Final output layer that converts hidden features back to image space

**Design Advantages:**
1. **Modular Reuse**: Avoids code duplication and keeps implementation clean
2. **Backward Compatibility**: Ensures complete consistency with original CDiT components
3. **Easy Maintenance**: Core component modifications automatically sync to hybrid model
4. **Independent Testing**: Enables independent testing of differences between CDiT and HybridCDiT

### Design Philosophy
- **Keep the Best, Retrieve Flexibly**: Retain the top-scored 40 frames, use flexible criteria to match relevant memories
- **Behavior-Oriented**: Focus on action behavior similarity rather than simple spatial distance
- **Dynamic Decay**: Implement natural forgetting and importance adjustment of memories
- **Adaptive Optimization**: Phase 2 adds zero-parameter adaptive retrieval weight system
- **Modular Reuse**: Fully utilize validated core components from `models.py`

## Dual-Standard Memory System (Phase 1 Implementation + Phase 2 Adaptive Optimization)

### 1. Storage Criteria (Intelligent Selection of Top-Scored 40 Frames)

The storage system uses a 100-point scoring system, always retaining the top-scored 40 frames. **No storage threshold limitation** - all frames participate in scoring, and the system automatically eliminates low-scoring frames.

#### Phase 1 Scoring Focus: Turning Behavior + Spatial Uniqueness

**Turning Behavior Detection (Core Features):**
- **Sharp Turn Landmarks** (+50.0 points): `storage_sharp_turn_weight` - Important navigation nodes (â‰¥26Â°)
- **Significant Turns** (+35.0 points): `storage_turn_weight` - Secondary navigation nodes (â‰¥14Â°)
- **Complex Maneuvers** (+15.0 points): `storage_complex_maneuver` - Difficult segments with turning while moving forward
- **Trivial Actions** (-8.0 points): `storage_trivial_penalty` - Reduces value of ordinary straight movement

**Spatial Uniqueness Detection:**
- **New Area Exploration** (+up to 60.0 points): `storage_spatial_weight` - Positions sufficiently far from existing memories (â‰¥4 meters)
- **Position Repetition** (-12.0 points): `storage_close_penalty` - Reduced value for repeated areas (<4 meters)
- **First Frame Bonus** (+40.0 points): `storage_first_frame_bonus` - Special importance of starting point

**Storage Strategy:**
- **Maximum Capacity**: 40 frames (`max_size`)
- **Storage Policy**: Retain top-scored 40 frames, no threshold limitation
- **Replacement Mechanism**: When cache is full, new frames compete with lowest-scoring frames, highest score wins

**Behavior Classification Thresholds (Phase 1 Focus):**
- **Significant Turn**: 0.25 radians (~14Â°) (`significant_turn_threshold`)
- **Sharp Turn**: 0.45 radians (~26Â°) (`sharp_turn_threshold`)
- **Linear Motion**: 0.2 meters (`linear_motion_threshold`)

**Phase 1 Design Advantages:**
- âœ… **Simplified Scoring**: Focus on turning behavior and spatial uniqueness
- âœ… **No Threshold Limitation**: All frames have chance to enter memory, avoiding missing important frames
- âœ… **Dynamic Competition**: Always maintain highest quality 40-frame memory
- âœ… **Simple Implementation**: Reduced complexity for easier debugging and optimization

### 2. Retrieval Criteria (Flexible Matching + Phase 2 Adaptive Optimization)

The retrieval system is responsible for finding the most helpful memories for current inference, using multi-factor weighted scoring.

#### Phase 2 Addition: Zero-Parameter Adaptive Weight System

**Core Concept**: Dynamically adjust retrieval weights based on current situation complexity, without requiring training parameters.

**Adaptive Logic**:
- **Stationary/Slow** (linear velocity < 0.1):
  - Action weight â†“ (0.2) - Actions are not obvious
  - Spatial weight â†‘â†‘ (0.6) - Mainly rely on position information
- **Fast Movement** (linear velocity > 0.4):
  - Action weight â†‘ (0.45) - Actions important, but not exceeding spatial
  - Spatial weight â†‘ (0.35) - Spatial remains the dominant factor
- **Large Turns** (turn angle > 0.3):
  - Action weight â†‘ (up to 0.6) - Turning behavior important
  - Spatial weight maintained (minimum 0.25) - Spatial information still critical

**Complexity Trigger Threshold**: Adaptive weights are enabled only when complexity > 0.3, simple cases use default weights.

#### Weight Distribution (Default + Adaptive)
- **Spatial Relevance**: Default 40% â†’ Adaptive 25-60% - **Primary factor**, spatial position most important for memory
- **Action Similarity**: Default 35% â†’ Adaptive 20-60% - Important factor, dynamically adjusted based on situation
- **Memory Value**: Default 20% â†’ Adaptive 15% - Important factor, basically remains stable
- **Usage Experience**: Default 5% â†’ Adaptive 5% - Auxiliary factor, remains stable

**Retrieval Parameters:**
- **Spatial Matching Radius**: 10.0 meters (`retrieval_spatial_radius`)
- **Default Retrieval Count**: 8 frames (`k=8`)

#### Action Similarity Algorithm
1. **Motion Magnitude Matching**: Similar movement distances (50% weight)
2. **Direction Consistency**: Similar movement directions (25% weight)
3. **Turn Behavior Classification**: Precise matching of straight/minor/turn/sharp turn behaviors (25% weight)

## Dynamic Decay System

### Fixed Memory Time Protection Mechanism (Consistent with Code Implementation)
- **Protection Period**: First 3 steps (`fixed_memory_time=3`) with no decay at all
- **Design Rationale**: New memories need time to prove their value, avoiding premature elimination

### Accelerated Decay Mechanism
Uses dynamic decay formula to implement natural forgetting of memories:
Decay Rate = Base Decay Rate Ã— (Acceleration Factor ^ Steps Exceeding Protection Period)
```python
# Actual decay calculation in code
excess_steps = unused_steps - fixed_memory_time  # Steps exceeding protection period
dynamic_decay_rate = base_decay_rate * (accelerated_decay_rate ** excess_steps)
new_score = max(old_score - dynamic_decay_rate, min_survival_score)
```

#### Parameter Settings (Actual Values in Code)
- **Base Decay Rate**: 1.2 points/step (`base_decay_rate`)
- **Acceleration Factor**: 1.4 (`accelerated_decay_rate`)
- **Minimum Survival Score**: 3.0 points (`min_survival_score`)
- **Maximum Score Limit**: 100.0 points (`max_score`)

#### Decay Examples (Actual Calculations)
- **Steps 1-3**: No decay (protection period, `unused_steps <= 3`)
- **Step 4**: Decay 1.2 Ã— 1.4Â¹ = 1.68 points
- **Step 5**: Decay 1.2 Ã— 1.4Â² = 2.35 points
- **Step 6**: Decay 1.2 Ã— 1.4Â³ = 3.29 points
- **Step 7**: Decay 1.2 Ã— 1.4â´ = 4.61 points
- **Continuous Acceleration**: Decay rate increases progressively, promoting forgetting

### Usage Reward Mechanism (Code Implementation)
- **Score Boost**: +5.0 points per use (`usage_boost`)
- **Cooldown Reset**: Re-enjoy 3-step protection period (`unused_steps=0`)
- **Usage Statistics**: Record usage frequency for retrieval weight calculation

```python
# Usage reward logic in code
self.scores[idx] = min(
    self.scores[idx] + config['usage_boost'],  # +5.0 points
    config['max_score']  # Not exceeding 100-point limit
)
self.unused_steps[idx] = 0  # Reset decay cooldown period
```

## Memory Statistics and Monitoring

The system provides complete statistical information for debugging and optimization:

### Basic Statistics
- Total memories / Maximum capacity
- Average score / Highest score / Lowest score
- Total usage count / Highest usage count

### Dynamic Decay Statistics
- **Protected Memories**: Number of memories enjoying protection
- **Decaying Memories**: Number of memories currently decaying
- **Average Unused Steps**: Health indicator of decay system
- **Maximum Unused Steps**: Longest unused memory

## Technical Implementation Details

### Architecture Integration
- **CDiT Backbone**: Maintains original self-attention and cross-attention
- **Selective Memory Attention**: Activates memory mechanism in latter layers (14-28)
- **Adaptive Activation**: Dynamically decides whether to use memory based on relevance scores

### Training and Inference Separation
- **Training Phase**: Disables memory system, focuses on basic capability learning
- **Inference Phase**: Fully activates memory system, utilizes historical experience

### Memory Management
- **Intelligent Caching**: LRU replacement strategy based on scoring
- **GPU Optimization**: Efficient tensor operations and memory reuse
- **Batch Processing Support**: Supports memory management for batch inference

## Configuration Parameter Description (Phase 1 Implementation + Phase 2 Adaptive Optimization)

All key parameters are centrally managed in `SCORING_CONFIG`, Phase 2 adds zero-parameter adaptive system:

### Storage Criteria Parameters (Phase 1: Turn and Spatial Focus)
```python
# Phase 1 configuration in hybrid_models.py
'storage_turn_weight': 35.0,           # Storage: Turn action importance
'storage_sharp_turn_weight': 50.0,     # Storage: Sharp turn additional weighting
'storage_spatial_weight': 30.0,        # Storage: Spatial uniqueness weight
'storage_complex_maneuver': 15.0,      # Storage: Complex maneuver bonus
'storage_trivial_penalty': -8.0,       # Storage: Trivial action penalty
'storage_close_penalty': -12.0,        # Storage: Too close position penalty
'storage_first_frame_bonus': 40.0,     # Storage: First frame starting point bonus
'storage_min_distance': 4.0,           # Storage: Minimum spatial spacing requirement (meters)
```

### Retrieval Criteria Parameters (Phase 2: Adaptive + Default Weights)
```python
# Default weights (used in simple cases)
'retrieval_action_weight': 0.35,       # Retrieval: Action similarity weight (important)
'retrieval_memory_weight': 0.20,       # Retrieval: Memory value weight (important)
'retrieval_spatial_weight': 0.40,      # Retrieval: Spatial relevance weight (primary) - Spatial position most important
'retrieval_usage_weight': 0.05,        # Retrieval: Usage experience weight (auxiliary)
'retrieval_spatial_radius': 10.0,      # Retrieval: Spatial matching radius (meters)

# Phase 2: Zero-parameter adaptive weights (automatically enabled in complex cases)
# Stationary/slow: action=0.2, spatial=0.6(dominant), memory=0.15, usage=0.05
# Fast movement: action=0.45, spatial=0.35(still dominant), memory=0.15, usage=0.05
# Large turns: action=0.6(highest), spatial=0.25(lowest but still important), memory=0.15, usage=0.05
# Complexity threshold: 0.3 (adaptive weights enabled above this value)
```

### Dynamic Decay System Parameters
```python
'usage_boost': 5.0,                    # Score boost per use
'fixed_memory_time': 3,                # Fixed memory time: No decay for first 3 steps
'base_decay_rate': 1.2,                # Base decay rate (points/step)
'accelerated_decay_rate': 1.4,         # Accelerated decay rate: Decay speed increase factor
'max_score': 100.0,                    # Maximum score limit
'min_survival_score': 3.0,             # Minimum retention score
```

### Behavior Classification Thresholds
```python
'significant_turn_threshold': 0.25,    # Significant turn threshold (radians, ~14Â°)
'sharp_turn_threshold': 0.45,          # Sharp turn threshold (radians, ~26Â°)
'linear_motion_threshold': 0.2,        # Linear motion threshold (meters)
```

### Cache Management Parameters (Phase 1 Simplified)
```python
# MemoryBuffer initialization parameters
max_size: int = 40,                     # Maximum cache capacity
# Note: Phase 1 removed min_score_threshold, retains top-scored 40 frames
```

### Model Architecture Parameters
```python
# HybridCDiT initialization parameters
memory_enabled: bool = True,            # Whether to enable memory mechanism
memory_buffer_size: int = 50,           # Memory cache size
memory_layers: List[int] = None         # Memory activation layers (default: latter half)

# Default memory layer configuration
if memory_layers is None:
    memory_layers = list(range(depth // 2, depth))  # Activate memory in latter half layers
```

## Performance Advantages

### Compared to Original CDiT
1. **Long-term Consistency**: Memory mechanism provides historical context
2. **Behavior Learning**: Learn optimal decisions from similar situations
3. **Adaptability**: Dynamic adjustment of memory importance

### Compared to Simple WorldMem
1. **Intelligent Filtering**: Not all frames are saved, only key frames are stored
2. **Precise Retrieval**: Based on behavior similarity rather than simple distance
3. **Dynamic Management**: Automatic forgetting and importance adjustment

## Usage Examples

### Basic Usage (Dependent on models.py)

```python
# Ensure models.py is in the same directory
from hybrid_models import HybridCDiT_L_2

# Create hybrid model (Phase 1)
model = HybridCDiT_L_2(
    memory_enabled=True, 
    memory_buffer_size=40,  # Fixed retention of top-scored 40 frames
    memory_layers=list(range(12, 24))  # Latter half layers of L model
)

# Automatic memory usage during inference
output = model(
    x=input_tensor,           # [N, C, H, W] Input image
    t=timesteps,              # [N] Diffusion timesteps
    y=actions,                # [N, 3] Action conditions [dx, dy, dyaw]
    x_cond=context_frames,    # [N, context_size, C, H, W] Context frames
    rel_t=relative_time,      # [N] Relative time
    current_pose=poses        # [N, 4] Current pose [x, y, z, yaw]
)

# Get memory statistics (consistent with get_memory_stats() in code)
stats = model.get_memory_stats()
print(f"Memory capacity: {stats['total_memories']}/{stats['max_capacity']}")
print(f"Average score: {stats['average_score']:.2f}")
print(f"Protected memories: {stats['protected_memories']}")
print(f"Decaying memories: {stats['decaying_memories']}")

# Phase 2 addition: Get adaptive scoring statistics
adaptive_stats = model.memory_buffer.get_adaptive_scoring_stats(poses[0], actions[0])
print(f"Complexity score: {adaptive_stats['complexity_score']:.3f}")
print(f"Using adaptive weights: {adaptive_stats['use_adaptive']}")
if adaptive_stats['use_adaptive']:
    print("Weight adjustments:", adaptive_stats['weight_differences'])
```

### Pure CDiT Mode (Compatibility Testing)

```python
# Disable memory to get identical behavior to original CDiT
model_cdit = HybridCDiT_L_2(memory_enabled=False)

# In this mode, current_pose parameter is not needed
output = model_cdit(x, t, y, x_cond, rel_t)
```

### Memory System Configuration Tuning (Phase 1)

```python
# Dynamically adjust scoring parameters after model creation
model = HybridCDiT_L_2(memory_enabled=True)

# Adjust storage strategy - emphasize turning behavior more
model.memory_buffer.SCORING_CONFIG.update({
    'storage_turn_weight': 40.0,        # Increase turn importance
    'storage_sharp_turn_weight': 60.0,  # Significantly increase sharp turn importance
    'storage_trivial_penalty': -12.0,   # More strict penalty for trivial actions
})

# Adjust retrieval strategy - focus more on spatial similarity
model.memory_buffer.SCORING_CONFIG.update({
    'retrieval_spatial_weight': 0.45,   # Enhance spatial similarity weight (dominant)
    'retrieval_action_weight': 0.30,    # Reduce action weight but still important
    'retrieval_memory_weight': 0.20,    # Moderate memory weight
    'retrieval_usage_weight': 0.05,     # Reduce usage weight
})

# Adjust decay strategy - longer protection period
model.memory_buffer.SCORING_CONFIG.update({
    'fixed_memory_time': 5,             # Extend protection period to 5 steps
    'usage_boost': 8.0,                 # Increase usage reward
})
```

### Memory System Monitoring (Phase 2 Enhancement)

```python
# Real-time monitoring of memory state during inference
for step, (x, t, y, x_cond, rel_t, pose) in enumerate(dataloader):
    output = model(x, t, y, x_cond, rel_t, pose)
    
    if step % 10 == 0:  # Monitor every 10 steps
        stats = model.get_memory_stats()
        print(f"Step {step}:")
        print(f"  Memory utilization: {stats['total_memories']}/{stats['max_capacity']}")
        print(f"  Average unused steps: {stats['avg_unused_steps']:.1f}")
        print(f"  Highest/Lowest scores: {stats['highest_score']:.1f}/{stats['lowest_score']:.1f}")
        
        # Phase 2 addition: Monitor adaptive scoring system
        if y is not None and pose is not None:
            adaptive_stats = model.memory_buffer.get_adaptive_scoring_stats(pose[0], y[0])
            print(f"  Complexity score: {adaptive_stats['complexity_score']:.3f}")
            print(f"  Adaptive weights: {'Enabled' if adaptive_stats['use_adaptive'] else 'Default'}")
            if adaptive_stats['use_adaptive']:
                weight_diff = adaptive_stats['weight_differences']
                print(f"  Weight adjustments: Action{weight_diff['action']:+.2f} Spatial{weight_diff['spatial']:+.2f}")
        
        # Check memory system health
        if stats['avg_unused_steps'] > 10:
            print("  Warning: Average unused steps too high, consider adjusting retrieval strategy")
        if stats['total_memories'] < stats['max_capacity'] * 0.5:
            print("  Note: Memory utilization low, consider lowering storage threshold")
```

## Project File Structure and Dependencies

### Core File Dependency Graph

```
models.py (Basic Components)
    â”œâ”€â”€ TimestepEmbedder
    â”œâ”€â”€ ActionEmbedder  
    â”œâ”€â”€ modulate
    â”œâ”€â”€ FinalLayer
    â””â”€â”€ CDiT (Original Model)
         â”‚
         â–¼
hybrid_models.py (Hybrid Architecture)
    â”œâ”€â”€ MemoryBuffer (Memory Cache System)
    â”œâ”€â”€ SelectiveMemoryAttention (Selective Memory Attention)
    â”œâ”€â”€ HybridCDiTBlock (Hybrid Transformer Block)
    â””â”€â”€ HybridCDiT (Main Model)
         â”‚
         â–¼
Application Layer Files
    â”œâ”€â”€ train.py (Training Script)
    â”œâ”€â”€ isolated_nwm_infer.py (Inference Script)
    â”œâ”€â”€ planning_eval.py (Planning Evaluation)
    â””â”€â”€ interactive_model.ipynb (Interactive Testing)
```

### Modular Design Principles

1. **Basic Component Reuse** (`models.py`)
   - Provides validated core embedding components
   - Maintains original CDiT implementation as baseline
   - Ensures numerical computation consistency

2. **Memory System Extension** (`hybrid_models.py`)
   - Builds hybrid architecture based on `models.py`
   - Adds intelligent memory management functionality
   - Maintains backward compatibility

3. **Application Layer Adaptation**
   - Existing scripts can seamlessly switch models
   - Supports A/B testing between CDiT and HybridCDiT
   - Progressive migration strategy

### Compatibility Guarantee

**Backward Compatibility:**
```python
# Using original CDiT
from models import CDiT_L_2
model_cdit = CDiT_L_2()

# Using hybrid model's CDiT mode (behavior completely identical)
from hybrid_models import HybridCDiT_L_2  
model_hybrid = HybridCDiT_L_2(memory_enabled=False)

# Both should produce identical outputs (within numerical precision)
assert torch.allclose(
    model_cdit(x, t, y, x_cond, rel_t),
    model_hybrid(x, t, y, x_cond, rel_t),
    rtol=1e-5
)
```

**Progressive Enhancement:**
```python
# Stage 1: Use CDiT mode to verify basic functionality
model = HybridCDiT_L_2(memory_enabled=False)

# Stage 2: Enable memory but no updates (observation only)
model = HybridCDiT_L_2(memory_enabled=True)
output = model(x, t, y, x_cond, rel_t, pose, update_memory=False)

# Stage 3: Fully enable memory system
model = HybridCDiT_L_2(memory_enabled=True)  
output = model(x, t, y, x_cond, rel_t, pose, update_memory=True)
```

## Tuning Recommendations (Phase 1)

### Strengthen Turn Behavior Detection
```python
# Storage strategy emphasizing turn behavior more
model.memory_buffer.SCORING_CONFIG.update({
    'storage_turn_weight': 45.0,        # Significantly increase turn weight
    'storage_sharp_turn_weight': 70.0,  # Greatly increase sharp turn weight
    'storage_trivial_penalty': -15.0,   # More strict penalty for trivial actions
    'storage_close_penalty': -15.0      # Stricter spatial deduplication
})
```

### Improve Retrieval Accuracy
```python
# Optimize retrieval weight distribution - spatial priority strategy
model.memory_buffer.SCORING_CONFIG.update({
    'retrieval_spatial_weight': 0.50,   # Significantly enhance spatial similarity weight
    'retrieval_action_weight': 0.30,    # Moderate action similarity weight
    'retrieval_memory_weight': 0.15,    # Reduce memory value influence
    'retrieval_usage_weight': 0.05,     # Minimize usage experience weight
    'retrieval_spatial_radius': 8.0,    # Reduce spatial radius for better precision
})
```

### Optimize Decay Strategy
```python
# Adjust memory lifecycle
model.memory_buffer.SCORING_CONFIG.update({
    'fixed_memory_time': 5,             # Extend protection period
    'base_decay_rate': 1.0,             # Reduce decay speed
    'usage_boost': 8.0,                 # Increase usage reward
})
```

## Important Notes

### 1. Dependency Requirements
- **Must ensure `models.py` is in the same directory or Python path**
- If moving `hybrid_models.py`, need to update import paths
- All core component modifications need to be made in `models.py`

### 2. Memory Management
- Memory cache will occupy GPU memory, adjust `memory_buffer_size` based on VRAM
- Memory system is active during inference, automatically disabled during training
- For long inference sessions, recommend periodic checking of memory usage statistics

### 3. Performance Considerations
- Memory activation adds ~15% computational overhead
- Can control memory usage layers by adjusting `memory_layers`
- During batch inference, memory is shared across batches

### 4. Debugging Tips
```python
# Enable detailed memory statistics output
stats = model.get_memory_stats()
for key, value in stats.items():
    print(f"{key}: {value}")

# Monitor memory activation frequency
if hasattr(model, 'memory_buffer'):
    print(f"Memory activation rate: {model.memory_buffer.usage_counts}")
```

## Future Extensions

### ðŸŽ¯ Phase 3 Extension Directions (Based on Success of First Two Phases)

1. **Viewpoint Diversity Detection**: Add angle difference scoring to avoid repetitive storage of similar viewpoints
2. **Height Advantage Recognition**: Include height factors, emphasize observation points and high ground
3. **Temporal Interval Filtering**: Avoid consecutive similar frames, improve temporal diversity
4. **Dynamic Threshold Adjustment**: Automatically adjust scoring weights based on cache utilization rate

### ðŸ”¬ Phase 4 Deep Optimization (Requires GPU Overhead Consideration)

5. **Lightweight Learnable Parameters**: Only key weights learnable (<1MB additional overhead)
6. **Semantic Memory**: Combine semantic similarity of visual features
7. **Multi-level Memory**: Short-term/medium-term/long-term memory hierarchical management
8. **Collaborative Memory**: Memory sharing mechanism between multiple agents

### ðŸ“‹ Implementation Priority (Based on First Two Phase Validation)

- **âœ… Completed**: Phase 1 - Turn behavior + Spatial uniqueness
- **âœ… Completed**: Phase 2 - Zero-parameter adaptive retrieval weight system
- **Short-term Implementation**: Phase 3 - Viewpoint diversity + Height advantage
- **Long-term Consideration**: Phase 4 - Deep learnable parameters (only after proving ROI)

### ðŸŽ¯ Phase 2 Achievement Validation

- âœ… **Zero-parameter Implementation**: No training required, dynamic retrieval weight adjustment
- âœ… **Smart Triggering**: Only enabled in complex situations, maintains efficiency in simple cases
- âœ… **Real-time Monitoring**: Provides detailed adaptive weight statistics
- âœ… **GPU-friendly**: Minimal computational overhead, mainly simple if-else logic
- âœ… **No Storage Impact**: Only optimizes retrieval logic, storage strategy remains stable

---

*Last Updated: August 5, 2025*
*Version: Hybrid Memory System v2.2 (Phase 1 + Phase 2 Adaptive Optimization)*
*Dependencies: models.py (Core Component Provider)*
        
        # Maintain buffer size
        if len(self.frames) > self.max_size:
            self.frames.pop(0)
            self.poses.pop(0)
            self.actions.pop(0)
            self.frame_indices.pop(0)
```

## Behavior-Driven Retrieval Mechanism

### Core Problem Solution

**Original Problem**: Retrieval based on state similarity leads to semantic errors
- **Moving Forward**: Select frames with similar orientation âœ“ Correct
- **When Turning**: Select frames with same orientation âœ— Wrong (needs turning experience, not straight-line experience)

**Solution**: Shift from state similarity to behavior relevance

### Intelligent Retrieval Algorithm

```python
def get_relevant_frames(self, current_pose, target_action=None, k=8):
    """
    Intelligent frame retrieval based on spatial proximity and behavior relevance
    
    Args:
        current_pose: Current position [x, y, z, yaw]
        target_action: Target action [delta_x, delta_y, delta_yaw]
        k: Number of frames to retrieve
    
    Returns:
        Top k most relevant historical frames
    """
    if len(self.frames) == 0:
        return None
        
    # 1. Spatial similarity calculation (80% weight)
    current_pos = current_pose[:3].cpu()
    memory_poses = torch.stack(self.poses)
    spatial_dists = torch.norm(memory_poses[:, :3] - current_pos[:3], dim=1)
    spatial_sims = torch.exp(-spatial_dists / 10.0)
    
    # 2. Behavior similarity calculation (20% weight)
    if target_action is not None and len(self.actions) > 0:
        target_action_cpu = target_action.cpu()
        memory_actions = torch.stack(self.actions)
        action_dists = torch.norm(memory_actions - target_action_cpu, dim=1)
        action_sims = torch.exp(-action_dists / 5.0)
        
        # Comprehensive scoring: spatial priority, behavior guidance
        similarities = 0.8 * spatial_sims + 0.2 * action_sims
    else:
        similarities = spatial_sims
    
    # 3. Select most relevant frames
    top_k_indices = torch.topk(similarities, min(k, len(similarities))).indices
    relevant_frames = [self.frames[i] for i in top_k_indices]
    return torch.stack(relevant_frames)
```

### Behavior Semantic Analysis

For action `[delta_x, delta_y, delta_yaw]` calculated from NoMaD trajectory data:

#### Intelligent Behavior Classification

**Problem**: Turn angle diversity causes simple distance matching to fail
- Need 90Â° right turn, history has 30Â° right turn and 90Â° left turn
- Simple distance: would incorrectly choose 30Â° right turn (distance 60Â°) over 90Â° left turn (distance 180Â°)

**Solution**: Behavior semantic classification rather than numerical matching

| Rotation Category | Angle Range | Behavior Characteristics | Matching Strategy |
|------------------|-------------|-------------------------|-------------------|
| **Straight** | \|yaw\| < 6Â° | Maintain direction | All straight movements match each other |
| **Minor** | 6Â° â‰¤ \|yaw\| < 17Â° | Small adjustment | Direction + magnitude matching |
| **Turn** | 17Â° â‰¤ \|yaw\| < 57Â° | Obvious turning | Direction priority, magnitude secondary |
| **Large Turn** | \|yaw\| â‰¥ 57Â° | Major turning | Direction priority, magnitude secondary |

#### Similarity Calculation Priority

```python
def compute_behavioral_similarity(target_action, memory_actions):
    # 1. Linear motion analysis (40% weight)
    magnitude_sims = exp(-|target_speed - memory_speed| / threshold)
    direction_sims = (dot_product + 1) / 2  # Movement direction similarity
    
    # 2. Rotation behavior classification (30% weight)  
    rotation_sims = classify_and_match_rotation(target_yaw, memory_yaw)
    
    # 3. Comprehensive scoring
    behavioral_sims = 0.4 * magnitude_sims + 0.3 * direction_sims + 0.3 * rotation_sims
    
    # Final combination with spatial similarity
    final_sims = 0.8 * spatial_sims + 0.2 * behavioral_sims
```

#### Actual Matching Effects (Improved Scoring Criteria)

**Scenario 1: Need 90Â° right turn**
- âœ… **Priority Selection**: 60Â° right turn(0.5 pts), 45Â° right turn(0.4 pts), 120Â° right turn(0.3 pts) (same direction turning)
- â­ **Score Difference**: Turn base score(0.3-0.5) << Straight base score(0.9-1.0)
- âŒ **Avoid Selection**: Straight(0.1 pts), 90Â° left turn(0.05 pts) (different behavior categories)

**Scenario 2: Need straight movement**
- âœ… **Priority Selection**: All straight experiences(1.0 pts) (delta_yaw â‰ˆ 0)
- â­ **Score Difference**: Straight vs turn has obvious score gap (1.0 vs 0.1)
- âŒ **Avoid Selection**: Any turning experience(0.1 pts)

**Scenario 3: Need minor adjustment (10Â° left turn)**
- âœ… **Optimal Match**: 5-15Â° left turn(0.7 pts) (same category same direction)
- âœ… **Suboptimal Match**: 20-30Â° left turn(0.5 pts) (same direction different category)
- âŒ **Avoid Selection**: Straight(0.1 pts), right turn(0.05 pts), large left turn(0.3 pts)

**Scoring Principles**:
- Straight category base score: 0.9-1.0 (High)
- Turn category base score: 0.3-0.7 (Medium-low)
- Cross-category penalty: Ã—0.1 (Strict distinction)

## Hybrid Architecture Advantages

### 1. Hierarchical Memory Activation

```python
class HybridCDiTBlock(nn.Module):
    def forward(self, x, c, x_cond, memory_frames=None, memory_activation_score=0.0):
        # 1. Self-attention (CDiT Standard)
        x = x + self.self_attention(x)
        
        # 2. Conditional cross-attention (CDiT Core Advantage)
        x = x + self.cross_attention(x, x_cond)
        
        # 3. Selective memory attention (WorldMem Enhancement)
        if self.enable_memory and memory_frames is not None:
            if memory_activation_score > 0.3:  # Intelligent activation
                x = x + self.memory_attention(x, memory_frames)
        
        # 4. MLP processing
        x = x + self.mlp(x)
        return x
```

### 2. Adaptive Memory Activation

```python
def compute_memory_activation_score(self, current_pose, action_magnitude):
    """
    Decide whether to activate memory based on situation complexity
    - Complex scenarios: Activate memory to get historical experience
    - Simple scenarios: Rely on immediate perception
    """
    # Based on action magnitude and scene complexity
    complexity_score = action_magnitude * 0.5
    
    # Based on memory relevance
    if len(self.memory_buffer.frames) > 10:
        memory_relevance = self.compute_memory_relevance(current_pose)
        complexity_score += memory_relevance * 0.5
    
    return min(complexity_score, 1.0)
```

## Training Integration

### Memory Usage Strategy

**Important Update**: Separate storage and inference logic, implement hierarchical memory mechanism

```python
def forward(self, x, t, y, x_cond, rel_t, current_pose=None, update_memory=True):
    # ... Model forward propagation ...
    
    # Memory retrieval: Only during inference and in specified layers
    if self.memory_enabled and current_pose is not None and not self.training:
        memory_frames = self.memory_buffer.get_relevant_frames(...)
    
    # Hierarchical processing:
    for i, block in enumerate(self.blocks):
        if i in self.memory_layers:
            # Later layers: Both store and use memory for inference
            x = block(x, c, x_cond, memory_frames, memory_activation_score)
        else:
            # Earlier layers (0-15): Only standard CDiT processing, no memory inference
            x = block(x, c, x_cond)
    
    # Memory storage: All layers perform storage during inference
    # Key design: Even if first 15 layers don't use memory inference, they still store memories
    if update_memory and self.memory_enabled and not self.training:
        current_action = y[0] if y is not None else None
        self.update_memory(x.detach(), current_pose[0], current_action)
    
    return x
```

### Hierarchical Memory Design

1. **Earlier Layers (0-15)**:
   - âœ… **Activate Memory Storage**: Continuously accumulate historical experience
   - âŒ **No Memory Inference**: Maintain CDiT's original processing capability
   - ðŸŽ¯ **Design Purpose**: Ensure memory system always works, providing rich material for subsequent layers

2. **Later Layers (memory_layers)**:
   - âœ… **Activate Memory Storage**: Continue accumulating experience
   - âœ… **Use Memory Inference**: Utilize historical experience to enhance generation
   - ðŸŽ¯ **Design Purpose**: Make intelligent decisions based on accumulated memory

### Training Advantages

1. **Pure GPU Training**: All computational operations stay on GPU, avoiding CPU-GPU data transfer
2. **No Memory Overhead**: No buffer usage during training, saving memory and computational resources
3. **Simple and Efficient**: Training logic basically same as standard CDiT, stable and reliable
4. **Inference Enhancement**: Memory mechanism only enabled during inference, providing additional contextual information
5. **Continuous Memory**: Continuous storage in earlier layers ensures memory system works uninterrupted

## Performance Optimization

### Vectorized Computation

All memory operations have been optimized with vectorization:

- **Original Loop Method**: ~23% computational overhead
- **After Vectorization Optimization**: ~15.6% computational overhead
- **Performance Improvement**: ~32% reduction in computation time

### Memory Management

- **Buffer Size**: Configurable (default 50 frames)
- **Activation Threshold**: Adaptive adjustment
- **Cleanup Strategy**: FIFO method to maintain buffer

## Configuration Description

### Model Configuration

```yaml
# config/nwm_hybrid.yaml
model: "CDiT-XL/2"
use_hybrid_model: true
memory_enabled: true
memory_buffer_size: 50
memory_layers: [0, 3, 6, 9]  # Activate memory in these layers

# Weight configuration
spatial_weight: 0.8  # Spatial similarity weight
action_weight: 0.2   # Behavior similarity weight
```

### Training Parameters

```python
# Use hybrid model
model = HybridCDiT_models["HybridCDiT-XL/2"](
    context_size=num_cond,
    input_size=latent_size,
    in_channels=4,
    memory_enabled=True,
    memory_buffer_size=50
)
```

## Practical Application Effects

### Scenario Examples

#### ðŸ¬ Warehouse Navigation
- **Task**: Navigate between shelves and make turns
- **Traditional Method**: May select straight-line experience with same orientation
- **Hybrid System**: Select turning experience from similar positions
- **Result**: More natural turning behavior

#### ðŸƒ Corridor Straight Movement
- **Task**: Maintain straight movement in long corridors
- **Traditional Method**: May select turning experience
- **Hybrid System**: Select historical experience of straight-line behavior
- **Result**: More stable straight-line control

## Technical Features

1. **Semantic Reasonableness**: Based on behavioral intent rather than state similarity
2. **Computational Efficiency**: Vectorized operations with performance optimization
3. **Adaptability**: Adjust memory activation based on scene complexity
4. **Extensibility**: Modular design, easy to customize
5. **Backward Compatibility**: Supports standard CDiT mode

## Summary

The WorldMem-CDiT hybrid memory system implements the design philosophy of "**Reasonableness over Similarity**", achieving:

- âœ… **Intelligent Memory Retrieval**: Select historical experience based on behavioral relevance
- âœ… **Efficient Computation**: Vectorized operations, minimal performance overhead
- âœ… **Semantic Consistency**: Ensure memory supports current behavioral intent
- âœ… **Adaptive Activation**: Intelligently use memory based on scene complexity

This design enables the model to call appropriate historical experience at the right time, significantly improving navigation intelligence and robustness.

## Intelligent Storage Mechanism Optimization

### Current Storage Strategy Analysis

**Current Status**: Currently all frames are unconditionally stored in memory buffer
- âœ… **Advantage**: Ensures no information is lost
- âŒ **Disadvantage**: May store large amounts of redundant or low-value information

### Key Position Detection

**Recommend implementing selective storage based on scene importance**:

1. **Large Turn Detection**:
   ```python
   # Detect significant turning actions
   if abs(delta_yaw) > SIGNIFICANT_TURN_THRESHOLD:
       should_store = True  # Visual information during turns is important
   ```

2. **Key Landmark Recognition**:
   ```python
   # Future extension: landmark detection based on visual features
   if detect_landmark(frame_features):
       should_store = True  # Landmark positions need to be remembered
   ```

3. **Behavior Change Points**:
   ```python
   # Detect behavior pattern changes
   if action_pattern_changed(current_action, previous_actions):
       should_store = True  # Behavior transition points are important
   ```

4. **Spatial Diversity**:
   ```python
   # Ensure spatial coverage diversity
   if spatial_diversity_score(current_pose, buffer_poses) > threshold:
       should_store = True  # New areas need to be remembered
   ```

### Storage Value Assessment Framework

```python
def compute_storage_value(frame, pose, action, buffer_state):
    """
    Calculate storage value score for frames
    
    Assessment dimensions:
    1. Behavior importance (turning, stopping, acceleration, etc.)
    2. Spatial novelty (whether reaching new areas)
    3. Temporal intervals (avoid consecutive similar frames)
    4. Buffer diversity (balance different types of experiences)
    """
    
    # 1. Behavior importance scoring
    behavior_score = evaluate_action_significance(action)
    
    # 2. Spatial novelty scoring
    spatial_score = evaluate_spatial_novelty(pose, buffer_state.poses)
    
    # 3. Temporal diversity scoring
    temporal_score = evaluate_temporal_diversity(buffer_state.timestamps)
    
    # 4. Buffer balance scoring
    balance_score = evaluate_buffer_balance(action, buffer_state.actions)
    
    # Comprehensive scoring
    storage_value = (0.4 * behavior_score + 
                    0.3 * spatial_score + 
                    0.2 * temporal_score + 
                    0.1 * balance_score)
    
    return storage_value
```

### Implementation Recommendations

1. **Phased Implementation**:
   - Phase 1: Simple filtering based on turn magnitude
   - Phase 2: Add spatial diversity consideration
   - Phase 3: Introduce visual landmark detection

2. **Dynamic Storage Threshold Adjustment**:
   ```python
   # Dynamically adjust storage threshold based on buffer utilization
   if buffer_utilization < 0.5:
       storage_threshold = 0.3  # Relaxed criteria
   elif buffer_utilization < 0.8:
       storage_threshold = 0.5  # Medium criteria
   else:
       storage_threshold = 0.7  # Strict criteria
   ```

3. **Priority Replacement Strategy**:
   - When buffer is full, prioritize replacing frames with lowest value scores
   - Preserve memories of key turning points and landmark positions

## Complete Normalization Update

### Core Modifications

To solve the yaw weight imbalance problem, **complete normalization** has been implemented:

**Problem**: Original yaw range [-3.14, 3.14] is similar to dx/dy range, causing yaw to dominate distance calculations in some cases, affecting training stability.

**Solution**: All action dimensions (dx, dy, dyaw) are uniformly normalized to [-1,1] range.

### Code Modifications

```python
# datasets.py & latent_dataset.py - Complete normalization
actions = normalize_data(actions, self.ACTION_STATS)  # All 3 dimensions
goal_pos = normalize_data(goal_pos, self.ACTION_STATS)  # All 3 dimensions

# hybrid_models.py - Current behavior classification thresholds (as implemented)
'significant_turn_threshold': 0.25,  # ~14Â° - Significant turn detection
'sharp_turn_threshold': 0.45,        # ~26Â° - Sharp turn detection  
'linear_motion_threshold': 0.2,      # Linear motion threshold
```

### Advantages

- âœ… **Weight Balance**: Eliminates yaw domination in distance calculations
- âœ… **Training Stability**: Consistent gradient scales across dimensions, reduces numerical instability
- âœ… **Behavior Classification**: Uses validated thresholds from actual implementation
- âœ… **Memory Consistency**: Both storage and retrieval use normalized values, logical consistency

### Current Data Pipeline

âš ï¸ **Current Implementation**: The project uses `latent_dataset.py` for training with pre-encoded latents

```bash
# Encode datasets for faster training
python scripts/encode_latents.py --dataset <dataset_name>

# Or encode all datasets
bash scripts/encode_all_datasets.sh

# Training uses latent_dataset.py automatically
python train.py --config config/nwm_cdit_xl.yaml
```
