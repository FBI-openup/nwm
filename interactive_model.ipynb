{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7475b4a4",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Initialize hybrid model, diffusion components, and memory system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cae14",
   "metadata": {},
   "source": [
    "## Model Path Configuration\n",
    "\n",
    "Configure your model checkpoint path and experiment settings below.\n",
    "\n",
    "**Available Options:**\n",
    "- **LT-NWM_L model**: Use if you have trained the full hybrid model\n",
    "- **CDiT-L 100k parameters**: Use existing checkpoint for testing\n",
    "- **Custom path**: Specify your own model file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95920ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Model Path Configuration =====\n",
    "# Please modify the following paths according to your actual situation\n",
    "\n",
    "# Option 1: Use trained LT-NWM_L model (if available)\n",
    "# MODEL_PATH = 'logs/lt-nwm_l/checkpoints/latest.pth.tar'\n",
    "\n",
    "# Option 2: Use existing CDiT-L 100k parameters (for testing)\n",
    "MODEL_PATH = 'logs/nwm_cdit_xl/cdit_l_100000.pth.tar'\n",
    "\n",
    "# Option 3: Other custom path\n",
    "# MODEL_PATH = 'path/to/your/model.pth.tar'\n",
    "\n",
    "# Experiment configuration name\n",
    "EXP_NAME = 'hybrid_l40s_inference_memory'  # Use hybrid configuration\n",
    "\n",
    "# Verify if model file exists\n",
    "import os\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚úÖ Model file found: {MODEL_PATH}\")\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(MODEL_PATH) / (1024**3)  # GB\n",
    "    print(f\"üìÅ File size: {file_size:.2f} GB\")\n",
    "else:\n",
    "    print(f\"‚ùå Model file not found: {MODEL_PATH}\")\n",
    "    print(\"Please check the path or train the model\")\n",
    "    \n",
    "    # List possible model files\n",
    "    possible_paths = [\n",
    "        'logs/nwm_cdit_xl/checkpoints/',\n",
    "        'logs/lt-nwm_l/checkpoints/',\n",
    "        'logs/',\n",
    "        'checkpoints/'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîç Searching for possible model files:\")\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"üìÇ {path}:\")\n",
    "            try:\n",
    "                files = [f for f in os.listdir(path) if f.endswith('.pth.tar') or f.endswith('.pth')]\n",
    "                if files:\n",
    "                    for file in sorted(files)[:5]:  # Show first 5 files\n",
    "                        full_path = os.path.join(path, file)\n",
    "                        size = os.path.getsize(full_path) / (1024**2)  # MB\n",
    "                        print(f\"   - {file} ({size:.1f} MB)\")\n",
    "                else:\n",
    "                    print(\"   (No .pth or .pth.tar files found)\")\n",
    "            except PermissionError:\n",
    "                print(\"   (Cannot access this directory)\")\n",
    "        else:\n",
    "            print(f\"üìÇ {path}: (Directory does not exist)\")\n",
    "\n",
    "print(f\"\\nCurrent configuration:\")\n",
    "print(f\"üéØ Experiment name: {EXP_NAME}\")\n",
    "print(f\"üìÑ Model path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d30f00",
   "metadata": {},
   "source": [
    "## Hybrid Model Configuration\n",
    "\n",
    "Display current hybrid model configuration information and system status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "# Fix: Use correct AutoencoderKL import path\n",
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL\n",
    "\n",
    "from diffusion import create_diffusion\n",
    "import sys\n",
    "import os\n",
    "# Fix: Use current working directory instead of __file__ in notebook\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(current_dir, 'scripts'))\n",
    "from scripts.isolated_nwm_infer import model_forward_wrapper\n",
    "from misc import transform\n",
    "# Modified: Use hybrid_models instead of models\n",
    "from hybrid_models import HybridCDiT_models\n",
    "# Removed unused TrainingDataset import\n",
    "\n",
    "# Check if model path and experiment name are defined\n",
    "try:\n",
    "    print(f\"Using model path: {MODEL_PATH}\")\n",
    "    print(f\"Using experiment config: {EXP_NAME}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Please run the model path configuration cell above first!\")\n",
    "    raise Exception(\"MODEL_PATH and EXP_NAME are not defined, please run configuration cell first\")\n",
    "\n",
    "with open(\"config/data_config.yaml\", \"r\") as f:\n",
    "    default_config = yaml.safe_load(f)\n",
    "config = default_config\n",
    "\n",
    "# Modified: Use hybrid configuration file\n",
    "with open(f'config/{EXP_NAME}.yaml', \"r\") as f:\n",
    "    user_config = yaml.safe_load(f)\n",
    "config.update(user_config)\n",
    "latent_size = config['image_size'] // 8\n",
    "\n",
    "print(\"Loading hybrid model with memory...\")\n",
    "# Modified: Use HybridCDiT_models and enable memory functionality\n",
    "model_name = config.get('model', 'CDiT-L/2')\n",
    "# Convert CDiT model name to HybridCDiT model name\n",
    "if 'CDiT-' in model_name:\n",
    "    hybrid_model_name = model_name.replace('CDiT-', 'HybridCDiT-')\n",
    "else:\n",
    "    hybrid_model_name = 'HybridCDiT-L/2'  # Default to L/2\n",
    "\n",
    "# Fix: Remove context_size parameter as it's already set in model definition\n",
    "model = HybridCDiT_models[hybrid_model_name](\n",
    "    input_size=latent_size, \n",
    "    memory_enabled=config.get('memory_enabled', True),\n",
    "    memory_buffer_size=config.get('memory_buffer_size', 100),\n",
    "    memory_layers=config.get('memory_layers', None)\n",
    ")\n",
    "\n",
    "print(f\"üîÑ Loading model parameters: {MODEL_PATH}\")\n",
    "ckp = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)\n",
    "missing_keys, unexpected_keys = model.load_state_dict(ckp[\"ema\"], strict=False)\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing keys: {len(missing_keys)} items\")\n",
    "    if len(missing_keys) <= 10:\n",
    "        for key in missing_keys:\n",
    "            print(f\"   - {key}\")\n",
    "    else:\n",
    "        print(f\"   (Showing first 10): {missing_keys[:10]}\")\n",
    "\n",
    "if unexpected_keys:\n",
    "    print(f\"‚ö†Ô∏è  Unexpected keys: {len(unexpected_keys)} items\")\n",
    "    if len(unexpected_keys) <= 10:\n",
    "        for key in unexpected_keys:\n",
    "            print(f\"   - {key}\")\n",
    "    else:\n",
    "        print(f\"   (Showing first 10): {unexpected_keys[:10]}\")\n",
    "\n",
    "if not missing_keys and not unexpected_keys:\n",
    "    print(\"‚úÖ Model parameters match perfectly\")\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Only compile if CUDA is available (compilation may cause issues on CPU)\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.compile(model)\n",
    "    print(\"‚úÖ Model compiled for GPU\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU - model compilation skipped\")\n",
    "\n",
    "# Initialize memory system\n",
    "if hasattr(model, 'reset_memory'):\n",
    "    model.reset_memory()\n",
    "    print(\"‚úÖ Memory system initialized\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model does not have memory system methods\")\n",
    "\n",
    "diffusion = create_diffusion(str(250))\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-ema\").to(device)\n",
    "latent_size = config['image_size'] // 8\n",
    "\n",
    "print(\"üéâ Model loading completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c46c1",
   "metadata": {},
   "source": [
    "## \udcbb CPU Simulation Mode - Skip GPU Model Loading\n",
    "\n",
    "Create mock versions to test code logic without actually loading large models, suitable for systems without GPU or insufficient memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CPU Simulation Mode - Do not load actual models, create mock objects for testing\n",
    "Suitable for systems without GPU or insufficient memory\n",
    "\"\"\"\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Check model path configuration\n",
    "try:\n",
    "    print(f\"‚úÖ Using model path: {MODEL_PATH}\")\n",
    "    print(f\"‚úÖ Using experiment config: {EXP_NAME}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Please run the model path configuration cell above first!\")\n",
    "    raise Exception(\"MODEL_PATH and EXP_NAME are not defined\")\n",
    "\n",
    "# Load configuration files\n",
    "with open(\"config/data_config.yaml\", \"r\") as f:\n",
    "    default_config = yaml.safe_load(f)\n",
    "config = default_config\n",
    "\n",
    "try:\n",
    "    with open(f'config/{EXP_NAME}.yaml', \"r\") as f:\n",
    "        user_config = yaml.safe_load(f)\n",
    "    config.update(user_config)\n",
    "    print(f\"‚úÖ Config loaded from: config/{EXP_NAME}.yaml\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è  Config file not found: config/{EXP_NAME}.yaml, using defaults\")\n",
    "\n",
    "latent_size = config['image_size'] // 8\n",
    "\n",
    "# Create mock model classes\n",
    "class MockHybridModel:\n",
    "    \"\"\"Mock hybrid model class for testing code logic\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_size, memory_enabled=True, memory_buffer_size=100):\n",
    "        self.latent_size = latent_size\n",
    "        self.memory_enabled = memory_enabled\n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        # Mock memory buffer\n",
    "        if memory_enabled:\n",
    "            self.memory_buffer = MockMemoryBuffer(memory_buffer_size)\n",
    "        else:\n",
    "            self.memory_buffer = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Mock parameters method, returns fake parameters for testing\"\"\"\n",
    "        # Create some fake parameter tensors\n",
    "        fake_params = [\n",
    "            torch.randn(1000, 512),  # Fake weight matrix\n",
    "            torch.randn(512),        # Fake bias\n",
    "            torch.randn(2048, 1024), # More fake parameters\n",
    "        ]\n",
    "        return fake_params\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set to evaluation mode\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "        self.device = device\n",
    "        return self\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Mock forward pass\"\"\"\n",
    "        # Return random noise as output\n",
    "        batch_size = 1\n",
    "        return torch.randn(batch_size, 4, self.latent_size, self.latent_size)\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Reset memory system\"\"\"\n",
    "        if self.memory_buffer:\n",
    "            self.memory_buffer.reset_memory()\n",
    "    \n",
    "    def store_frame_in_memory(self, frame, pose, action):\n",
    "        \"\"\"Store frame in memory\"\"\"\n",
    "        if self.memory_buffer:\n",
    "            self.memory_buffer.add_frame(frame, pose, action)\n",
    "    \n",
    "    def get_memory_stats(self):\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        if self.memory_buffer:\n",
    "            return self.memory_buffer.get_stats()\n",
    "        return {'memory_count': 0, 'max_capacity': 0, 'usage_ratio': 0.0}\n",
    "\n",
    "class MockMemoryBuffer:\n",
    "    \"\"\"Mock memory buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.frames = []\n",
    "        self.poses = []\n",
    "        self.actions = []\n",
    "        self.scores = []\n",
    "        self.usage_counts = []\n",
    "        self.unused_steps = []\n",
    "        self.frame_indices = []\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def add_frame(self, frame, pose, action):\n",
    "        \"\"\"Add new frame\"\"\"\n",
    "        if len(self.frames) < self.max_size:\n",
    "            self.frames.append(frame)\n",
    "            self.poses.append(pose)\n",
    "            self.actions.append(action)\n",
    "            self.scores.append(np.random.uniform(0.5, 1.0))  # Random score\n",
    "            self.usage_counts.append(0)\n",
    "            self.unused_steps.append(0)\n",
    "            self.frame_indices.append(self.current_index)\n",
    "        self.current_index += 1\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Reset memory\"\"\"\n",
    "        self.frames.clear()\n",
    "        self.poses.clear()\n",
    "        self.actions.clear()\n",
    "        self.scores.clear()\n",
    "        self.usage_counts.clear()\n",
    "        self.unused_steps.clear()\n",
    "        self.frame_indices.clear()\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics\"\"\"\n",
    "        return {\n",
    "            'memory_count': len(self.frames),\n",
    "            'max_capacity': self.max_size,\n",
    "            'usage_ratio': len(self.frames) / self.max_size if self.max_size > 0 else 0.0\n",
    "        }\n",
    "\n",
    "class MockDiffusion:\n",
    "    \"\"\"Mock diffusion model\"\"\"\n",
    "    \n",
    "    def p_sample_loop(self, model_fn, shape, noise, **kwargs):\n",
    "        \"\"\"Mock sampling loop\"\"\"\n",
    "        # Return randomly generated samples\n",
    "        return torch.randn(*shape)\n",
    "\n",
    "class MockVAE:\n",
    "    \"\"\"Mock VAE model\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Mock encoding\"\"\"\n",
    "        class MockLatentDist:\n",
    "            def sample(self):\n",
    "                # Return mock latent representation\n",
    "                return torch.randn(x.shape[0], 4, x.shape[2]//8, x.shape[3]//8)\n",
    "        \n",
    "        class MockOutput:\n",
    "            latent_dist = MockLatentDist()\n",
    "        \n",
    "        return MockOutput()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Mock decoding\"\"\"\n",
    "        class MockSample:\n",
    "            @property\n",
    "            def sample(self):\n",
    "                # Decode from latent space back to image space\n",
    "                return torch.randn(z.shape[0], 3, z.shape[2]*8, z.shape[3]*8)\n",
    "        \n",
    "        return MockSample()\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "print(\"üèóÔ∏è  Creating mock models for CPU testing...\")\n",
    "\n",
    "# Create mock model and components\n",
    "model_name = config.get('model', 'CDiT-L/2')\n",
    "hybrid_model_name = model_name.replace('CDiT-', 'HybridCDiT-') if 'CDiT-' in model_name else 'HybridCDiT-L/2'\n",
    "\n",
    "model = MockHybridModel(\n",
    "    latent_size=latent_size,\n",
    "    memory_enabled=config.get('memory_enabled', True),\n",
    "    memory_buffer_size=config.get('memory_buffer_size', 100)\n",
    ")\n",
    "\n",
    "# Mock device\n",
    "device = 'cpu'  # Force use CPU\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Initialize memory system\n",
    "if hasattr(model, 'reset_memory'):\n",
    "    model.reset_memory()\n",
    "    print(\"‚úÖ Mock memory system initialized\")\n",
    "\n",
    "# Create mock diffusion and VAE\n",
    "diffusion = MockDiffusion()\n",
    "vae = MockVAE(device)\n",
    "\n",
    "print(\"üéâ Mock model setup completed!\")\n",
    "print(\"\\nüìù Note: This is a simulation version for testing code logic\")\n",
    "print(\"   - All outputs are randomly generated\")\n",
    "print(\"   - Will not produce real navigation results\")\n",
    "print(\"   - But can verify code structure and interface correctness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220ce70",
   "metadata": {},
   "source": [
    "## System Requirements & Performance Notes\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Recommended**: NVIDIA GPU with 8GB+ VRAM\n",
    "- **Minimum**: 16GB+ system RAM for CPU fallback\n",
    "\n",
    "**Automatic Optimizations:**\n",
    "- GPU detection and device switching\n",
    "- Conditional model compilation (GPU only)\n",
    "- Mixed precision inference (when supported)\n",
    "\n",
    "**Known Limitations:**\n",
    "- CPU inference may be extremely slow\n",
    "- Large memory models may cause kernel crashes on limited hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model and configuration are loaded\n",
    "try:\n",
    "    # Type hints to help Pylance recognize variable types\n",
    "    from typing import Any, Dict, Union\n",
    "    \n",
    "    # Verify necessary variables exist\n",
    "    if 'model' not in globals():\n",
    "        raise NameError(\"model is not defined - please run the model loading cell first\")\n",
    "    if 'config' not in globals():\n",
    "        raise NameError(\"config is not defined - please run the model loading cell first\")\n",
    "    \n",
    "    # Display Hybrid Model Configuration Information\n",
    "    print(\"=== Hybrid Model Configuration ===\")\n",
    "    print(f\"Experiment name: {EXP_NAME}\")\n",
    "    print(f\"Model path: {MODEL_PATH}\")\n",
    "    \n",
    "    # Safely get hybrid_model_name\n",
    "    if 'hybrid_model_name' in globals():\n",
    "        print(f\"Model type: {hybrid_model_name}\")\n",
    "    else:\n",
    "        print(\"Model type: Mock Model (CPU Testing)\")\n",
    "    \n",
    "    print(f\"Input size: {latent_size}\")\n",
    "    print(f\"Context size: {config.get('context_size', 'Not specified')}\")\n",
    "    print(f\"Memory enabled: {config.get('memory_enabled', True)}\")\n",
    "    print(f\"Memory buffer size: {config.get('memory_buffer_size', 100)}\")\n",
    "    print(f\"Memory layers: {config.get('memory_layers', 'Auto')}\")\n",
    "    print(f\"Adaptive memory weights: {config.get('adaptive_memory_weights', True)}\")\n",
    "\n",
    "    # Display model parameter count (safe handling)\n",
    "    try:\n",
    "        params = list(model.parameters())\n",
    "        total_params = sum(p.numel() for p in params)\n",
    "        trainable_params = sum(p.numel() for p in params if hasattr(p, 'requires_grad') and p.requires_grad)\n",
    "        print(f\"\\nModel Parameters:\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    except Exception as param_error:\n",
    "        print(f\"\\nModel Parameters: Unable to calculate ({param_error})\")\n",
    "\n",
    "    # Display memory system configuration (safe handling)\n",
    "    try:\n",
    "        if hasattr(model, 'memory_buffer') and getattr(model, 'memory_buffer', None) is not None:\n",
    "            memory_buffer = getattr(model, 'memory_buffer')\n",
    "            print(f\"\\nMemory System:\")\n",
    "            print(f\"Max buffer size: {getattr(memory_buffer, 'max_size', 'Unknown')}\")\n",
    "            \n",
    "            # Safely get frames list length\n",
    "            frames = getattr(memory_buffer, 'frames', [])\n",
    "            print(f\"Current memory count: {len(frames) if frames is not None else 0}\")\n",
    "            \n",
    "            if hasattr(memory_buffer, 'adaptive_scorer'):\n",
    "                print(\"Adaptive scoring system: Enabled\")\n",
    "        else:\n",
    "            print(\"\\nMemory System: Not enabled\")\n",
    "    except Exception as memory_error:\n",
    "        print(f\"\\nMemory System: Error accessing memory system ({memory_error})\")\n",
    "\n",
    "    # Display retrieval weights configuration (if any)\n",
    "    if isinstance(config, dict) and 'retrieval_weights' in config:\n",
    "        print(f\"\\nRetrieval weights configuration:\")\n",
    "        retrieval_weights = config['retrieval_weights']\n",
    "        if isinstance(retrieval_weights, dict):\n",
    "            for key, value in retrieval_weights.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "    print(\"\\n=== Configuration loading completed ===\\n\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please run the model loading cell above first!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error displaying configuration: {e}\")\n",
    "    print(\"Some configuration details may be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70023fe3",
   "metadata": {},
   "source": [
    "## Choose Starting Image\n",
    "\n",
    "Select an initial environment image from the gallery below to begin navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_pil_image(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def load_internet_image(url):\n",
    "    from torchvision import transforms\n",
    "    _transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "    ])\n",
    "    img = url_to_pil_image(url)\n",
    "    x_start = _transform(img)\n",
    "    return x_start.unsqueeze(0).expand(config['context_size'], x_start.shape[0], x_start.shape[1], x_start.shape[2])\n",
    "\n",
    "# Jupyter Notebook Cell\n",
    "\n",
    "# List of image links\n",
    "image_links = [\n",
    "    # Official dataset images\n",
    "    #'https://raw.githubusercontent.com/amirbar/amirbar.github.io/refs/heads/master/images/recon.png',\n",
    "    #'https://raw.githubusercontent.com/amirbar/amirbar.github.io/refs/heads/master/images/scand.png',\n",
    "    #'https://raw.githubusercontent.com/amirbar/amirbar.github.io/refs/heads/master/images/sacson.png',\n",
    "    #'https://raw.githubusercontent.com/amirbar/amirbar.github.io/refs/heads/master/images/tartan.png',\n",
    "    \n",
    "    # Custom warehouse images \n",
    "    # Known images different from official website\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/frame.png',\n",
    "    \n",
    "    # Unknown warehouse environments (for testing generalization)\n",
    "    'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/realistic-warehouse-photo1.png',\n",
    "    'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/warehouse2.png',\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/obstracle+warehouse3.jpg',\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/Resized.jpg',\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/realwarehouse.png',\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/Low.png',\n",
    "\t\n",
    "    # Unknown environments from internet\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/livingroom.jpg',\n",
    "   \n",
    "    # Unknown environments from official website\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/chateau.png',\n",
    "    # Unknown from roboflow\n",
    "    #'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/cornel.png'\n",
    "]\n",
    "\n",
    "# Output widget to hold the selected link\n",
    "output = widgets.Output()\n",
    "x_start_link = None  # This will hold the selected link\n",
    "\n",
    "# Function to handle image click\n",
    "def on_image_click(link):\n",
    "    global x_start_link\n",
    "    x_start_link = link\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"Selected image link:\\n{x_start_link}\")\n",
    "\n",
    "# Create HBox of images\n",
    "image_buttons = []\n",
    "for link in image_links:\n",
    "    img = widgets.Button(\n",
    "        description='click',\n",
    "        layout=widgets.Layout(width='150px', height='20px', padding='0'),\n",
    "        style={'button_color': 'lightgray'}\n",
    "    )\n",
    "\n",
    "    img._dom_classes += ('image-button',)\n",
    "    img_link = link  # capture current link in closure\n",
    "\n",
    "    def on_click(b, link=img_link):\n",
    "        on_image_click(link)\n",
    "\n",
    "    img.on_click(on_click)\n",
    "\n",
    "    # Embed image using HTML style\n",
    "    img_html = f'<img src=\"{link}\" width=\"150px\" height=\"150px\">'\n",
    "    img_html_widget = widgets.HTML(value=img_html)\n",
    "    image_buttons.append(widgets.VBox([img_html_widget, img]))\n",
    "\n",
    "# Display the gallery\n",
    "display(widgets.HBox(image_buttons))\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdf3ef",
   "metadata": {},
   "source": [
    "## Interactive Navigation\n",
    "\n",
    "Use the buttons below to control navigation and observe memory-enhanced predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved CPU-friendly interactive navigation system\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Check necessary variables\n",
    "if 'x_start_link' not in globals() or x_start_link is None:\n",
    "    print(\"‚ö†Ô∏è  Please select an image from the gallery above first!\")\n",
    "    x_start_link = 'https://raw.githubusercontent.com/FBI-openup/nwm/main/warehouse%20Images/realistic-warehouse-photo1.png'\n",
    "    print(f\"Using default image: {x_start_link}\")\n",
    "\n",
    "print(f\"Selected starting image: {x_start_link}\")\n",
    "\n",
    "# Navigation command definitions\n",
    "commands = {\n",
    "    'Forward': [1, 0, 0],\n",
    "    'Rotate Right': [0, 0, -0.5],\n",
    "    'Rotate Left': [0, 0, 0.5],\n",
    "    'Backward': [-1, 0, 0]\n",
    "}\n",
    "\n",
    "# Initialize state variables\n",
    "navigation_state = {\n",
    "    'current_pose': torch.zeros(4).to(device),  # [x, y, z, yaw]\n",
    "    'step_count': 0,\n",
    "    'trajectory': [(0, 0)],  # Record trajectory\n",
    "    'actions_taken': [],\n",
    "    'memory_usage': []\n",
    "}\n",
    "\n",
    "def mock_image_generation(action_name):\n",
    "    \"\"\"Generate mock navigation images\"\"\"\n",
    "    # Create simple visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Left: Mock generated environment image\n",
    "    ax1.set_title(f\"Generated View - Step {navigation_state['step_count']}\")\n",
    "    \n",
    "    # Create simple warehouse environment simulation\n",
    "    warehouse_bg = np.random.uniform(0.3, 0.7, (224, 224, 3))\n",
    "    \n",
    "    # Add some \"shelves\" and \"corridors\"\n",
    "    for i in range(3):\n",
    "        x_start = 40 + i * 60\n",
    "        warehouse_bg[50:170, x_start:x_start+20] = [0.8, 0.6, 0.4]  # Shelf color\n",
    "    \n",
    "    # Add floor texture\n",
    "    warehouse_bg[170:, :] = [0.5, 0.5, 0.6]\n",
    "    \n",
    "    # Add visual changes based on action\n",
    "    if action_name == 'Forward':\n",
    "        warehouse_bg[100:120, 100:120] = [1.0, 0.8, 0.8]  # Forward highlight\n",
    "    elif 'Rotate' in action_name:\n",
    "        warehouse_bg[80:140, 80:140] = warehouse_bg[80:140, 80:140] * 0.9  # Rotation effect\n",
    "    \n",
    "    ax1.imshow(warehouse_bg)\n",
    "    ax1.set_xlabel(f\"Action: {action_name}\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Right: Trajectory and status display\n",
    "    ax2.set_title(\"Navigation Trajectory & Memory Status\")\n",
    "    ax2.set_xlim(-5, 5)\n",
    "    ax2.set_ylim(-5, 5)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Draw trajectory\n",
    "    if len(navigation_state['trajectory']) > 1:\n",
    "        trajectory = np.array(navigation_state['trajectory'])\n",
    "        ax2.plot(trajectory[:, 0], trajectory[:, 1], 'b-', alpha=0.7, linewidth=2, label='Path')\n",
    "        ax2.scatter(trajectory[:, 0], trajectory[:, 1], c='blue', s=30, alpha=0.7)\n",
    "    \n",
    "    # Draw current position\n",
    "    current_pos = navigation_state['trajectory'][-1]\n",
    "    ax2.scatter(current_pos[0], current_pos[1], c='red', s=100, marker='o', label='Current Position')\n",
    "    \n",
    "    # Draw orientation\n",
    "    yaw = navigation_state['current_pose'][3].item()\n",
    "    dx = 0.5 * np.cos(yaw)\n",
    "    dy = 0.5 * np.sin(yaw)\n",
    "    ax2.arrow(current_pos[0], current_pos[1], dx, dy, head_width=0.2, head_length=0.2, fc='red', ec='red')\n",
    "    \n",
    "    # Add memory usage information\n",
    "    memory_stats = navigation_state['memory_usage']\n",
    "    if memory_stats:\n",
    "        latest_memory = memory_stats[-1]\n",
    "        ax2.text(0.02, 0.98, f\"Memory: {latest_memory['memory_count']}/{latest_memory['max_capacity']}\", \n",
    "                transform=ax2.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('X Position')\n",
    "    ax2.set_ylabel('Y Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def update_navigation_state(action_name):\n",
    "    \"\"\"Update navigation state\"\"\"\n",
    "    action = commands[action_name]\n",
    "    action_tensor = torch.tensor(action).to(device)\n",
    "    \n",
    "    # Update pose\n",
    "    if action_name == 'Forward':\n",
    "        navigation_state['current_pose'][0] += 0.5  # Move forward in x\n",
    "    elif action_name == 'Backward':\n",
    "        navigation_state['current_pose'][0] -= 0.5  # Move backward in x\n",
    "    elif action_name == 'Rotate Left':\n",
    "        navigation_state['current_pose'][3] += 0.3  # Rotate left (yaw)\n",
    "    elif action_name == 'Rotate Right':\n",
    "        navigation_state['current_pose'][3] -= 0.3  # Rotate right (yaw)\n",
    "    \n",
    "    # Update trajectory\n",
    "    x, y = navigation_state['current_pose'][0].item(), navigation_state['current_pose'][1].item()\n",
    "    navigation_state['trajectory'].append((x, y))\n",
    "    \n",
    "    # Update step count\n",
    "    navigation_state['step_count'] += 1\n",
    "    navigation_state['actions_taken'].append(action_name)\n",
    "    \n",
    "    # Simulate memory usage (add to mock memory buffer)\n",
    "    if hasattr(model, 'memory_buffer'):\n",
    "        # Create mock observation frame\n",
    "        mock_frame = torch.randn(4, 28, 28)  # Mock latent features\n",
    "        model.store_frame_in_memory(mock_frame, navigation_state['current_pose'], action_tensor)\n",
    "        \n",
    "        # Record memory state\n",
    "        memory_stats = model.get_memory_stats()\n",
    "        navigation_state['memory_usage'].append(memory_stats)\n",
    "\n",
    "def reset_navigation():\n",
    "    \"\"\"Reset navigation state\"\"\"\n",
    "    navigation_state['current_pose'] = torch.zeros(4).to(device)\n",
    "    navigation_state['step_count'] = 0\n",
    "    navigation_state['trajectory'] = [(0, 0)]\n",
    "    navigation_state['actions_taken'] = []\n",
    "    navigation_state['memory_usage'] = []\n",
    "    \n",
    "    if hasattr(model, 'reset_memory'):\n",
    "        model.reset_memory()\n",
    "    \n",
    "    print(\"üîÑ Navigation reset!\")\n",
    "    mock_image_generation(\"Reset\")\n",
    "\n",
    "# Create improved control interface\n",
    "def create_navigation_controls():\n",
    "    \"\"\"Create navigation control interface\"\"\"\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_action_click(action_name):\n",
    "        with output:\n",
    "            output.clear_output(wait=True)\n",
    "            print(f\"üéÆ Action: {action_name}\")\n",
    "            update_navigation_state(action_name)\n",
    "            mock_image_generation(action_name)\n",
    "            \n",
    "            # Display status information\n",
    "            print(f\"Step: {navigation_state['step_count']}\")\n",
    "            pose = navigation_state['current_pose']\n",
    "            print(f\"Position: x={pose[0]:.2f}, y={pose[1]:.2f}, yaw={pose[3]:.2f}\")\n",
    "            \n",
    "            if navigation_state['memory_usage']:\n",
    "                latest_memory = navigation_state['memory_usage'][-1]\n",
    "                print(f\"Memory: {latest_memory['memory_count']}/{latest_memory['max_capacity']} \"\n",
    "                      f\"({latest_memory['usage_ratio']:.1%} full)\")\n",
    "    \n",
    "    def on_reset_click(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            reset_navigation()\n",
    "    \n",
    "    # Create buttons\n",
    "    buttons = []\n",
    "    for action in ['Forward', 'Rotate Left', 'Rotate Right', 'Backward']:\n",
    "        btn = widgets.Button(\n",
    "            description=f\"üéØ {action}\",\n",
    "            layout=widgets.Layout(width='150px', margin='2px'),\n",
    "            style={'button_color': 'lightgreen'}\n",
    "        )\n",
    "        btn.on_click(lambda b, action=action: on_action_click(action))\n",
    "        buttons.append(btn)\n",
    "    \n",
    "    reset_btn = widgets.Button(\n",
    "        description=\"üîÑ Reset\",\n",
    "        layout=widgets.Layout(width='150px', margin='2px'),\n",
    "        style={'button_color': 'orange'}\n",
    "    )\n",
    "    reset_btn.on_click(on_reset_click)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üïπÔ∏è Navigation Controls (CPU Mock Mode)</h3>\"),\n",
    "        widgets.HBox(buttons[:2]),\n",
    "        widgets.HBox(buttons[2:] + [reset_btn]),\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return controls\n",
    "\n",
    "# Initialize and display control interface\n",
    "print(\"üöÄ Initializing CPU-friendly navigation system...\")\n",
    "navigation_controls = create_navigation_controls()\n",
    "display(navigation_controls)\n",
    "\n",
    "# Display initial state\n",
    "reset_navigation()\n",
    "\n",
    "print(\"\\n‚úÖ Navigation system ready!\")\n",
    "print(\"üí° Tip: This is CPU simulation mode, all images are programmatically generated visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5016619",
   "metadata": {},
   "source": [
    "## Memory System Status\n",
    "\n",
    "Monitor the current state and performance of the memory buffer system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory system status monitoring\n",
    "def display_memory_status():\n",
    "    \"\"\"Display detailed memory system status\"\"\"\n",
    "    if not hasattr(model, 'memory_buffer') or model.memory_buffer is None:\n",
    "        print(\"Memory system not enabled\")\n",
    "        return\n",
    "    \n",
    "    buffer = model.memory_buffer\n",
    "    print(\"=== Memory System Status ===\")\n",
    "    print(f\"Current memory count: {len(buffer.frames)}/{buffer.max_size}\")\n",
    "    print(f\"Capacity usage: {len(buffer.frames)/buffer.max_size:.1%}\")\n",
    "    \n",
    "    if len(buffer.frames) > 0:\n",
    "        print(f\"Memory score range: {min(buffer.scores):.1f} - {max(buffer.scores):.1f}\")\n",
    "        print(f\"Average usage count: {sum(buffer.usage_counts)/len(buffer.usage_counts):.1f}\")\n",
    "        \n",
    "        # Display recent memories\n",
    "        print(\"\\nRecent 5 memories:\")\n",
    "        recent_indices = sorted(range(len(buffer.frame_indices)), \n",
    "                              key=lambda i: buffer.frame_indices[i], reverse=True)[:5]\n",
    "        for i in recent_indices:\n",
    "            print(f\"  Frame {buffer.frame_indices[i]}: score={buffer.scores[i]:.1f}, \"\n",
    "                  f\"used={buffer.usage_counts[i]}x, unused_steps={buffer.unused_steps[i]}\")\n",
    "\n",
    "# Create button to display memory status\n",
    "status_button = widgets.Button(description=\"Show Memory Status\", style={'button_color': 'lightblue'})\n",
    "status_output = widgets.Output()\n",
    "\n",
    "def show_status(b):\n",
    "    with status_output:\n",
    "        status_output.clear_output()\n",
    "        display_memory_status()\n",
    "\n",
    "status_button.on_click(show_status)\n",
    "display(status_button)\n",
    "display(status_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved memory system status monitoring\n",
    "def display_memory_status():\n",
    "    \"\"\"Display detailed memory system status - compatible with mock models\"\"\"\n",
    "    try:\n",
    "        if not hasattr(model, 'memory_buffer') or getattr(model, 'memory_buffer', None) is None:\n",
    "            print(\"‚ùå Memory system not enabled\")\n",
    "            return\n",
    "        \n",
    "        buffer = getattr(model, 'memory_buffer')\n",
    "        print(\"=== Memory System Status ===\")\n",
    "        \n",
    "        # Safely get memory information\n",
    "        frames = getattr(buffer, 'frames', [])\n",
    "        max_size = getattr(buffer, 'max_size', 0)\n",
    "        \n",
    "        print(f\"Current memory count: {len(frames)}/{max_size}\")\n",
    "        print(f\"Capacity usage: {len(frames)/max_size:.1%}\" if max_size > 0 else \"Capacity usage: N/A\")\n",
    "        \n",
    "        if len(frames) > 0:\n",
    "            # Safely get score information\n",
    "            scores = getattr(buffer, 'scores', [])\n",
    "            usage_counts = getattr(buffer, 'usage_counts', [])\n",
    "            \n",
    "            if scores:\n",
    "                print(f\"Memory score range: {min(scores):.1f} - {max(scores):.1f}\")\n",
    "            \n",
    "            if usage_counts:\n",
    "                avg_usage = sum(usage_counts)/len(usage_counts)\n",
    "                print(f\"Average usage count: {avg_usage:.1f}\")\n",
    "            \n",
    "            # Display recent memories\n",
    "            print(\"\\nRecent memories:\")\n",
    "            frame_indices = getattr(buffer, 'frame_indices', list(range(len(frames))))\n",
    "            \n",
    "            # Show most recent 5 memory entries\n",
    "            recent_count = min(5, len(frames))\n",
    "            for i in range(recent_count):\n",
    "                idx = len(frames) - 1 - i  # Start from newest\n",
    "                frame_idx = frame_indices[idx] if idx < len(frame_indices) else idx\n",
    "                score = scores[idx] if idx < len(scores) else 0.0\n",
    "                usage = usage_counts[idx] if idx < len(usage_counts) else 0\n",
    "                \n",
    "                print(f\"  Frame {frame_idx}: score={score:.1f}, used={usage}x\")\n",
    "        else:\n",
    "            print(\"No memories stored yet\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error accessing memory system: {e}\")\n",
    "        print(\"Memory system may not be properly initialized\")\n",
    "\n",
    "# Create interactive button - improved version\n",
    "def create_memory_status_widget():\n",
    "    \"\"\"Create memory status display widget\"\"\"\n",
    "    status_button = widgets.Button(\n",
    "        description=\"üîç Show Memory Status\", \n",
    "        style={'button_color': 'lightblue'},\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    status_output = widgets.Output()\n",
    "\n",
    "    def show_status(b):\n",
    "        with status_output:\n",
    "            status_output.clear_output()\n",
    "            display_memory_status()\n",
    "\n",
    "    status_button.on_click(show_status)\n",
    "    \n",
    "    # Create description text\n",
    "    info_text = widgets.HTML(\n",
    "        value=\"<p><b>Memory System Monitor</b><br/>Click button to view current memory buffer status</p>\"\n",
    "    )\n",
    "    \n",
    "    return widgets.VBox([info_text, status_button, status_output])\n",
    "\n",
    "# Display widget\n",
    "memory_widget = create_memory_status_widget()\n",
    "display(memory_widget)\n",
    "\n",
    "print(\"‚úÖ Memory status monitor created\")\n",
    "print(\"üí° Tip: In simulation mode, memory data is randomly generated for interface testing only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa99a6",
   "metadata": {},
   "source": [
    "## üß™ Comprehensive Testing: File Path & Memory System Validation\n",
    "\n",
    "Specifically test whether your modified file structure and memory mechanisms are correctly configured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a5a6c",
   "metadata": {},
   "source": [
    "# Navigation Video Generation\n",
    "\n",
    "Generate an animated video from the recorded navigation sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e944a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(preds['video'])\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=500)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load from dataset\n",
    "# dataloaders = {}\n",
    "\n",
    "# for dataset_name in config[\"datasets\"]:\n",
    "#     data_config = config[\"datasets\"][dataset_name]\n",
    "#     for data_split_type in [\"test\"]: #[\"test\"]:\n",
    "#         dataset = TrainingDataset(\n",
    "#             data_folder=data_config[\"data_folder\"],\n",
    "#             data_split_folder=data_config[data_split_type],\n",
    "#             dataset_name=dataset_name,\n",
    "#             image_size=config[\"image_size\"],\n",
    "#             min_dist_cat=config[\"distance\"][\"min_dist_cat\"],\n",
    "#             max_dist_cat=config[\"distance\"][\"max_dist_cat\"],\n",
    "#             len_traj_pred=config[\"len_traj_pred\"],\n",
    "#             context_size=config[\"context_size\"],\n",
    "#             normalize=config[\"normalize\"],\n",
    "#             goals_per_obs=1,\n",
    "#             transform=transform,\n",
    "#             predefined_index=None,\n",
    "#             traj_stride=1,\n",
    "#         )\n",
    "#         dataloaders[f\"{dataset_name}_{data_split_type}\"] = dataset\n",
    "#         print(f\"Dataset: {dataset_name} ({data_split_type}), size: {len(dataset)}\")\n",
    "\n",
    "# Load from dataset\n",
    "# ds = dataloaders['recon_test'] # scand_test,\n",
    "# x, _, _ = ds[np.random.randint(len(ds))]\n",
    "# x_start = x[:config[\"context_size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f63cf",
   "metadata": {},
   "source": [
    "## üîß Notebook Cell Diagnostics & Repair\n",
    "\n",
    "If you encounter issues such as cell backgrounds dimming or inability to edit cells, please use the methods below for repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b92d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook Cell State Repair Tool\n",
    "Run this cell when encountering the following issues:\n",
    "1. Code background dimming\n",
    "2. Unable to edit cells\n",
    "3. Cell locked after Copilot modifications\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "from IPython.display import display, HTML, Javascript\n",
    "\n",
    "def fix_notebook_state():\n",
    "    \"\"\"Fix notebook state issues\"\"\"\n",
    "    print(\"üîß Starting notebook state repair...\")\n",
    "    \n",
    "    # 1. Memory cleanup\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ Memory cleanup completed\")\n",
    "    \n",
    "    # 2. Check kernel state\n",
    "    print(f\"üêç Python version: {sys.version}\")\n",
    "    print(f\"üíæ Memory usage: {sys.getsizeof(globals())} bytes\")\n",
    "    \n",
    "    # 3. Reset IPython display settings\n",
    "    try:\n",
    "        from IPython.core.display import clear_output\n",
    "        print(\"‚úÖ IPython display system normal\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è IPython display system exception: {e}\")\n",
    "    \n",
    "    # 4. Check variable state\n",
    "    important_vars = ['MODEL_PATH', 'EXP_NAME', 'model', 'device', 'config']\n",
    "    for var in important_vars:\n",
    "        if var in globals():\n",
    "            print(f\"‚úÖ {var}: Defined\")\n",
    "        else:\n",
    "            print(f\"‚ùå {var}: Not defined\")\n",
    "    \n",
    "    print(\"\\nüéØ Repair suggestions:\")\n",
    "    print(\"1. If cell background is dimmed, double-click the cell to enter edit mode\")\n",
    "    print(\"2. Press Ctrl+Z to undo recent modifications\")\n",
    "    print(\"3. Press Ctrl+Shift+P and search for 'Reload Window'\")\n",
    "    print(\"4. If issues persist, restart kernel (Ctrl+Shift+P -> 'Restart Kernel')\")\n",
    "\n",
    "# Run repair\n",
    "fix_notebook_state()\n",
    "\n",
    "# JavaScript code to fix frontend display issues\n",
    "display(Javascript(\"\"\"\n",
    "// Fix cell display issues\n",
    "console.log('Fixing cell state...');\n",
    "\n",
    "// Remove possible locked state\n",
    "document.querySelectorAll('.cell').forEach(cell => {\n",
    "    cell.classList.remove('readonly', 'locked', 'disabled');\n",
    "    cell.style.opacity = '1';\n",
    "    cell.style.backgroundColor = '';\n",
    "});\n",
    "\n",
    "// Reactivate edit functionality\n",
    "document.querySelectorAll('.jp-Cell').forEach(cell => {\n",
    "    cell.style.opacity = '1';\n",
    "    cell.style.filter = 'none';\n",
    "});\n",
    "\n",
    "console.log('Cell state repair completed');\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n‚ú® State repair completed! If issues persist, try the following operations:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6ad05",
   "metadata": {},
   "source": [
    "### üöÄ Manual Repair Steps\n",
    "\n",
    "If automatic repair is ineffective, please follow these steps:\n",
    "\n",
    "#### Method 1: Reactivate Cells\n",
    "1. **Double-click the dimmed cell** - Force entry into edit mode\n",
    "2. **Press `Esc` then `Enter`** - Exit and re-enter edit mode\n",
    "3. **Press `Ctrl+Z`** - Undo recent modifications\n",
    "\n",
    "#### Method 2: VS Code Command Palette\n",
    "1. **Press `Ctrl+Shift+P`** to open command palette\n",
    "2. Search and execute the following commands:\n",
    "   - `Developer: Reload Window` - Reload window\n",
    "   - `Jupyter: Restart Kernel` - Restart kernel\n",
    "   - `Jupyter: Clear All Outputs` - Clear all outputs\n",
    "\n",
    "#### Method 3: Kernel Restart\n",
    "1. Click the **\"Restart\"** button at the top\n",
    "2. Or press `Ctrl+Shift+P` ‚Üí `Jupyter: Restart Kernel and Clear All Outputs`\n",
    "\n",
    "#### Method 4: Check Extensions\n",
    "1. Ensure the following extensions are enabled:\n",
    "   - **Jupyter Extension Pack**\n",
    "   - **GitHub Copilot**\n",
    "   - **Python Extension**\n",
    "\n",
    "#### Preventive Measures\n",
    "- **Save frequently**: Use `Ctrl+S` often\n",
    "- **Small modifications**: Avoid large code changes at once\n",
    "- **Backup important code**: Copy important code blocks to text files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083b122",
   "metadata": {},
   "source": [
    "## üß™ Comprehensive CPU Testing Suite\n",
    "\n",
    "Integrated testing framework for validating file structure, configuration loading, memory system, and overall integration without requiring GPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üß™ Comprehensive CPU Testing Suite\n",
    "Integrated testing framework for systems without GPU\n",
    "Tests: File structure, Configuration loading, Memory system, Integration validation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import matplotlib.patches as mpatches\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Javascript\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# ===========================\n",
    "# 1. MOCK MODEL CLASSES\n",
    "# ===========================\n",
    "\n",
    "class MockHybridModel:\n",
    "    \"\"\"Mock hybrid model class for testing code logic\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_size, memory_enabled=True, memory_buffer_size=100):\n",
    "        self.latent_size = latent_size\n",
    "        self.memory_enabled = memory_enabled\n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        # Mock memory buffer\n",
    "        if memory_enabled:\n",
    "            self.memory_buffer = MockMemoryBuffer(memory_buffer_size)\n",
    "        else:\n",
    "            self.memory_buffer = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Mock parameters method, returns fake parameters for testing\"\"\"\n",
    "        fake_params = [\n",
    "            torch.randn(1000, 512),  # Fake weight matrix\n",
    "            torch.randn(512),        # Fake bias\n",
    "            torch.randn(2048, 1024), # More fake parameters\n",
    "        ]\n",
    "        return fake_params\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set to evaluation mode\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "        self.device = device\n",
    "        return self\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Mock forward pass\"\"\"\n",
    "        batch_size = 1\n",
    "        return torch.randn(batch_size, 4, self.latent_size, self.latent_size)\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Reset memory system\"\"\"\n",
    "        if self.memory_buffer:\n",
    "            self.memory_buffer.reset_memory()\n",
    "    \n",
    "    def store_frame_in_memory(self, frame, pose, action):\n",
    "        \"\"\"Store frame in memory\"\"\"\n",
    "        if self.memory_buffer:\n",
    "            self.memory_buffer.add_frame(frame, pose, action)\n",
    "    \n",
    "    def get_memory_stats(self):\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        if self.memory_buffer:\n",
    "            return self.memory_buffer.get_stats()\n",
    "        return {'memory_count': 0, 'max_capacity': 0, 'usage_ratio': 0.0}\n",
    "\n",
    "class MockMemoryBuffer:\n",
    "    \"\"\"Mock memory buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.frames = []\n",
    "        self.poses = []\n",
    "        self.actions = []\n",
    "        self.scores = []\n",
    "        self.usage_counts = []\n",
    "        self.unused_steps = []\n",
    "        self.frame_indices = []\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def add_frame(self, frame, pose, action):\n",
    "        \"\"\"Add new frame\"\"\"\n",
    "        if len(self.frames) < self.max_size:\n",
    "            self.frames.append(frame)\n",
    "            self.poses.append(pose)\n",
    "            self.actions.append(action)\n",
    "            self.scores.append(np.random.uniform(0.5, 1.0))  # Random score\n",
    "            self.usage_counts.append(0)\n",
    "            self.unused_steps.append(0)\n",
    "            self.frame_indices.append(self.current_index)\n",
    "        self.current_index += 1\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Reset memory\"\"\"\n",
    "        self.frames.clear()\n",
    "        self.poses.clear()\n",
    "        self.actions.clear()\n",
    "        self.scores.clear()\n",
    "        self.usage_counts.clear()\n",
    "        self.unused_steps.clear()\n",
    "        self.frame_indices.clear()\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics\"\"\"\n",
    "        return {\n",
    "            'memory_count': len(self.frames),\n",
    "            'max_capacity': self.max_size,\n",
    "            'usage_ratio': len(self.frames) / self.max_size if self.max_size > 0 else 0.0\n",
    "        }\n",
    "\n",
    "class MockDiffusion:\n",
    "    \"\"\"Mock diffusion model\"\"\"\n",
    "    \n",
    "    def p_sample_loop(self, model_fn, shape, noise, **kwargs):\n",
    "        \"\"\"Mock sampling loop\"\"\"\n",
    "        return torch.randn(*shape)\n",
    "\n",
    "class MockVAE:\n",
    "    \"\"\"Mock VAE model\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Mock encoding\"\"\"\n",
    "        class MockLatentDist:\n",
    "            def sample(self):\n",
    "                return torch.randn(x.shape[0], 4, x.shape[2]//8, x.shape[3]//8)\n",
    "        \n",
    "        class MockOutput:\n",
    "            latent_dist = MockLatentDist()\n",
    "        \n",
    "        return MockOutput()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Mock decoding\"\"\"\n",
    "        class MockSample:\n",
    "            @property\n",
    "            def sample(self):\n",
    "                return torch.randn(z.shape[0], 3, z.shape[2]*8, z.shape[3]*8)\n",
    "        \n",
    "        return MockSample()\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "# ===========================\n",
    "# 2. TESTING FUNCTIONS\n",
    "# ===========================\n",
    "\n",
    "def test_file_structure():\n",
    "    \"\"\"Test 1: Validate file path structure\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üóÇÔ∏è  File Structure Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check critical directories\n",
    "    critical_paths = {\n",
    "        \"Model file\": MODEL_PATH,\n",
    "        \"Config directory\": \"config/\",\n",
    "        \"Logs directory\": \"logs/\",\n",
    "        \"Scripts directory\": \"scripts/\",\n",
    "        \"Data directory\": \"data/\",\n",
    "        \"Data splits directory\": \"data_splits/\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, path in critical_paths.items():\n",
    "        exists = os.path.exists(path)\n",
    "        results[name] = exists\n",
    "        \n",
    "        status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"{status} {name}: {path}\")\n",
    "        \n",
    "        if exists and os.path.isdir(path):\n",
    "            try:\n",
    "                items = os.listdir(path)\n",
    "                print(f\"   üìÅ Contains {len(items)} items\")\n",
    "                if len(items) <= 5:\n",
    "                    for item in items[:3]:\n",
    "                        print(f\"      - {item}\")\n",
    "                else:\n",
    "                    print(f\"      - {items[0]}, {items[1]}, ... ({len(items)} total)\")\n",
    "            except PermissionError:\n",
    "                print(\"   ‚ö†Ô∏è  Cannot access directory contents\")\n",
    "        elif exists and os.path.isfile(path):\n",
    "            size_mb = os.path.getsize(path) / (1024**2)\n",
    "            print(f\"   üìÑ File size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nüìä Results: {sum(results.values())}/{len(results)} paths exist\")\n",
    "    return results\n",
    "\n",
    "def test_config_loading():\n",
    "    \"\"\"Test 2: Configuration file loading\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚öôÔ∏è  Configuration File Loading Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    config_files = [\n",
    "        \"config/data_config.yaml\",\n",
    "        f\"config/{EXP_NAME}.yaml\",\n",
    "        \"config/data_hyperparams_plan.yaml\",\n",
    "        \"config/eval_config.yaml\"\n",
    "    ]\n",
    "    \n",
    "    config_results = {}\n",
    "    \n",
    "    for config_file in config_files:\n",
    "        try:\n",
    "            if os.path.exists(config_file):\n",
    "                with open(config_file, 'r') as f:\n",
    "                    config_data = yaml.safe_load(f)\n",
    "                \n",
    "                print(f\"‚úÖ {config_file}\")\n",
    "                print(f\"   üìù Contains {len(config_data)} configuration items\")\n",
    "                \n",
    "                # Show important configurations\n",
    "                important_keys = ['image_size', 'context_size', 'memory_enabled', 'memory_buffer_size']\n",
    "                for key in important_keys:\n",
    "                    if key in config_data:\n",
    "                        print(f\"   - {key}: {config_data[key]}\")\n",
    "                \n",
    "                config_results[config_file] = True\n",
    "            else:\n",
    "                print(f\"‚ùå {config_file} - File does not exist\")\n",
    "                config_results[config_file] = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {config_file} - Loading error: {e}\")\n",
    "            config_results[config_file] = False\n",
    "    \n",
    "    return config_results\n",
    "\n",
    "def test_memory_storage_mechanism():\n",
    "    \"\"\"Test 3: Memory storage mechanism\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üß† Memory Storage Mechanism Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Reset memory buffer\n",
    "    model.reset_memory()\n",
    "    initial_count = len(model.memory_buffer.frames)\n",
    "    print(f\"Initial memory count: {initial_count}\")\n",
    "    \n",
    "    # Simulate storing several observation frames\n",
    "    test_frames = []\n",
    "    test_poses = []\n",
    "    test_actions = []\n",
    "    \n",
    "    print(\"\\nüìù Storing test frames...\")\n",
    "    for i in range(5):\n",
    "        # Create mock observation frame (latent features)\n",
    "        frame = torch.randn(4, latent_size, latent_size)\n",
    "        pose = torch.tensor([i * 0.5, 0.0, 0.0, i * 0.1])  # x, y, z, yaw\n",
    "        action = torch.tensor([1.0, 0.0, 0.0])  # Forward action\n",
    "        \n",
    "        print(f\"Storing frame {i+1}: pose=({pose[0]:.1f}, {pose[1]:.1f}, {pose[3]:.1f})\")\n",
    "        model.store_frame_in_memory(frame, pose, action)\n",
    "        \n",
    "        test_frames.append(frame)\n",
    "        test_poses.append(pose)\n",
    "        test_actions.append(action)\n",
    "    \n",
    "    # Verify storage results\n",
    "    final_count = len(model.memory_buffer.frames)\n",
    "    print(f\"\\nüìä Storage results:\")\n",
    "    print(f\"   Before: {initial_count} frames\")\n",
    "    print(f\"   After: {final_count} frames\")\n",
    "    print(f\"   Added: {final_count - initial_count} frames\")\n",
    "    \n",
    "    # Check memory buffer contents\n",
    "    buffer = model.memory_buffer\n",
    "    if hasattr(buffer, 'scores') and len(buffer.scores) > 0:\n",
    "        print(f\"   Score range: {min(buffer.scores):.2f} - {max(buffer.scores):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'initial_count': initial_count,\n",
    "        'final_count': final_count,\n",
    "        'frames_added': final_count - initial_count\n",
    "    }\n",
    "\n",
    "def test_memory_retrieval_criteria():\n",
    "    \"\"\"Test 4: Memory retrieval criteria\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéØ Memory Retrieval Criteria Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get current configured retrieval weights\n",
    "    retrieval_config = config.get('retrieval_weights', {})\n",
    "    \n",
    "    print(\"üìã Configured retrieval criteria:\")\n",
    "    total_weight = sum(retrieval_config.values())\n",
    "    print(f\"Weight sum: {total_weight} {'‚úÖ' if abs(total_weight - 1.0) < 0.01 else '‚ö†Ô∏è Not equal to 1.0'}\")\n",
    "    \n",
    "    for criterion, weight in retrieval_config.items():\n",
    "        percentage = weight * 100\n",
    "        importance = \"High\" if weight > 0.3 else \"Medium\" if weight > 0.15 else \"Low\"\n",
    "        print(f\"   ‚Ä¢ {criterion}: {weight} ({percentage:.1f}%) - {importance} importance\")\n",
    "    \n",
    "    # Simulate memory retrieval process\n",
    "    buffer = model.memory_buffer\n",
    "    \n",
    "    if len(buffer.frames) > 0:\n",
    "        print(\"\\nüîç Memory retrieval simulation:\")\n",
    "        \n",
    "        # Current query\n",
    "        current_pose = torch.tensor([2.0, 0.0, 0.0, 0.2])\n",
    "        current_action = torch.tensor([1.0, 0.0, 0.0])\n",
    "        \n",
    "        print(f\"Query position: ({current_pose[0]:.1f}, {current_pose[1]:.1f}, {current_pose[3]:.1f})\")\n",
    "        \n",
    "        # Calculate similarity scores for each memory\n",
    "        similarities = []\n",
    "        for i, (stored_pose, stored_action) in enumerate(zip(buffer.poses[:5], buffer.actions[:5])):\n",
    "            # Spatial similarity (closer distance = higher similarity)\n",
    "            spatial_dist = torch.norm(current_pose[:3] - stored_pose[:3]).item()\n",
    "            spatial_sim = 1.0 / (1.0 + spatial_dist)\n",
    "            \n",
    "            # Action similarity (cosine similarity)\n",
    "            action_sim = torch.cosine_similarity(\n",
    "                current_action.unsqueeze(0), \n",
    "                stored_action.unsqueeze(0)\n",
    "            ).item()\n",
    "            \n",
    "            # Memory score (stored score)\n",
    "            memory_score = buffer.scores[i] if i < len(buffer.scores) else 0.5\n",
    "            \n",
    "            # Usage frequency (less used = higher weight)\n",
    "            usage_count = buffer.usage_counts[i] if i < len(buffer.usage_counts) else 0\n",
    "            usage_weight = 1.0 / (1.0 + usage_count)\n",
    "            \n",
    "            # Combined score\n",
    "            total_score = (\n",
    "                retrieval_config.get('spatial_weight', 0.4) * spatial_sim +\n",
    "                retrieval_config.get('action_weight', 0.3) * action_sim +\n",
    "                retrieval_config.get('memory_weight', 0.2) * memory_score +\n",
    "                retrieval_config.get('usage_weight', 0.1) * usage_weight\n",
    "            )\n",
    "            \n",
    "            similarities.append({\n",
    "                'index': i,\n",
    "                'spatial_sim': spatial_sim,\n",
    "                'action_sim': action_sim,\n",
    "                'memory_score': memory_score,\n",
    "                'usage_weight': usage_weight,\n",
    "                'total_score': total_score\n",
    "            })\n",
    "            \n",
    "            print(f\"   Memory{i}: spatial={spatial_sim:.2f}, action={action_sim:.2f}, \"\n",
    "                  f\"memory={memory_score:.2f}, usage={usage_weight:.2f} ‚Üí total={total_score:.2f}\")\n",
    "        \n",
    "        # Sort and show most relevant memories\n",
    "        similarities.sort(key=lambda x: x['total_score'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüèÜ Most relevant memories:\")\n",
    "        for i, sim in enumerate(similarities[:3]):\n",
    "            print(f\"   Rank {i+1}: Memory{sim['index']} (score: {sim['total_score']:.3f})\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Memory buffer is empty, cannot test retrieval\")\n",
    "    \n",
    "    return retrieval_config\n",
    "\n",
    "def test_integration():\n",
    "    \"\"\"Test 5: Integration test\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üîó Integration Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"üìä Environment status:\")\n",
    "    print(f\"   Python device: {device}\")\n",
    "    print(f\"   Model type: {type(model).__name__}\")\n",
    "    print(f\"   Diffusion model: {type(diffusion).__name__}\")\n",
    "    print(f\"   VAE model: {type(vae).__name__}\")\n",
    "    print(f\"   Latent size: {latent_size}\")\n",
    "    \n",
    "    # Test model call chain\n",
    "    print(\"\\nüîÑ Model call chain test:\")\n",
    "    try:\n",
    "        # Simulate complete inference\n",
    "        mock_observation = torch.randn(1, 4, 3, 224, 224)  # batch, time, channels, height, width\n",
    "        mock_action = torch.tensor([[1.0, 0.0, 0.0]])\n",
    "        mock_pose = torch.zeros(4)\n",
    "        \n",
    "        print(\"   1. VAE encoding...\")\n",
    "        encoded = vae.encode(mock_observation.flatten(0, 1))\n",
    "        latent = encoded.latent_dist.sample()\n",
    "        print(f\"      ‚úÖ Latent shape: {latent.shape}\")\n",
    "        \n",
    "        print(\"   2. Diffusion sampling...\")\n",
    "        noise = torch.randn_like(latent)\n",
    "        denoised = diffusion.p_sample_loop(\n",
    "            model.forward, \n",
    "            noise.shape, \n",
    "            noise\n",
    "        )\n",
    "        print(f\"      ‚úÖ Denoised shape: {denoised.shape}\")\n",
    "        \n",
    "        print(\"   3. VAE decoding...\")\n",
    "        decoded = vae.decode(denoised)\n",
    "        output = decoded.sample\n",
    "        print(f\"      ‚úÖ Output shape: {output.shape}\")\n",
    "        \n",
    "        print(\"   4. Memory update...\")\n",
    "        model.store_frame_in_memory(latent[0], mock_pose, mock_action[0])\n",
    "        memory_stats = model.get_memory_stats()\n",
    "        print(f\"      ‚úÖ Memory status: {memory_stats['memory_count']}/{memory_stats['max_capacity']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Integration test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_mock_navigation_demo():\n",
    "    \"\"\"Create mock navigation demonstration\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéÆ Mock Navigation Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Navigation commands\n",
    "    commands = {\n",
    "        'Forward': [1, 0, 0],\n",
    "        'Rotate Right': [0, 0, -0.5],\n",
    "        'Rotate Left': [0, 0, 0.5],\n",
    "        'Backward': [-1, 0, 0]\n",
    "    }\n",
    "    \n",
    "    # Initialize navigation state\n",
    "    navigation_state = {\n",
    "        'current_pose': torch.zeros(4).to(device),  # [x, y, z, yaw]\n",
    "        'step_count': 0,\n",
    "        'trajectory': [(0, 0)],\n",
    "        'actions_taken': [],\n",
    "        'memory_usage': []\n",
    "    }\n",
    "    \n",
    "    def mock_navigation_step(action_name):\n",
    "        \"\"\"Simulate one navigation step\"\"\"\n",
    "        action = commands[action_name]\n",
    "        action_tensor = torch.tensor(action).to(device)\n",
    "        \n",
    "        # Update pose\n",
    "        if action_name == 'Forward':\n",
    "            navigation_state['current_pose'][0] += 0.5\n",
    "        elif action_name == 'Backward':\n",
    "            navigation_state['current_pose'][0] -= 0.5\n",
    "        elif action_name == 'Rotate Left':\n",
    "            navigation_state['current_pose'][3] += 0.3\n",
    "        elif action_name == 'Rotate Right':\n",
    "            navigation_state['current_pose'][3] -= 0.3\n",
    "        \n",
    "        # Update trajectory\n",
    "        x, y = navigation_state['current_pose'][0].item(), navigation_state['current_pose'][1].item()\n",
    "        navigation_state['trajectory'].append((x, y))\n",
    "        \n",
    "        # Update step count\n",
    "        navigation_state['step_count'] += 1\n",
    "        navigation_state['actions_taken'].append(action_name)\n",
    "        \n",
    "        # Store in memory\n",
    "        mock_frame = torch.randn(4, 28, 28)\n",
    "        model.store_frame_in_memory(mock_frame, navigation_state['current_pose'], action_tensor)\n",
    "        \n",
    "        # Record memory state\n",
    "        memory_stats = model.get_memory_stats()\n",
    "        navigation_state['memory_usage'].append(memory_stats)\n",
    "        \n",
    "        return navigation_state\n",
    "    \n",
    "    # Simulate sequence of actions\n",
    "    action_sequence = ['Forward', 'Forward', 'Rotate Right', 'Forward', 'Rotate Left', 'Forward']\n",
    "    \n",
    "    print(\"üéØ Simulating navigation sequence:\")\n",
    "    for i, action in enumerate(action_sequence):\n",
    "        state = mock_navigation_step(action)\n",
    "        pose = state['current_pose']\n",
    "        memory = state['memory_usage'][-1]\n",
    "        \n",
    "        print(f\"   Step {i+1}: {action}\")\n",
    "        print(f\"      Position: x={pose[0]:.1f}, y={pose[1]:.1f}, yaw={pose[3]:.1f}\")\n",
    "        print(f\"      Memory: {memory['memory_count']}/{memory['max_capacity']} ({memory['usage_ratio']:.1%})\")\n",
    "    \n",
    "    # Create simple visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.set_title(\"Mock Navigation Trajectory\")\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot trajectory\n",
    "    trajectory = np.array(navigation_state['trajectory'])\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    \n",
    "    # Mark start and end\n",
    "    ax.scatter(trajectory[0, 0], trajectory[0, 1], c='green', s=100, marker='s', label='Start')\n",
    "    ax.scatter(trajectory[-1, 0], trajectory[-1, 1], c='red', s=100, marker='*', label='End')\n",
    "    \n",
    "    # Add action annotations\n",
    "    for i, action in enumerate(action_sequence):\n",
    "        if i < len(trajectory) - 1:\n",
    "            ax.annotate(f\"{i+1}:{action[:3]}\", \n",
    "                       (trajectory[i+1, 0], trajectory[i+1, 1]),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlabel('X Position')\n",
    "    ax.set_ylabel('Y Position')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return navigation_state\n",
    "\n",
    "def fix_notebook_state():\n",
    "    \"\"\"Fix notebook state issues\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üîß Notebook State Repair\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Memory cleanup\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ Memory cleanup completed\")\n",
    "    \n",
    "    # 2. Check kernel state\n",
    "    print(f\"üêç Python version: {sys.version}\")\n",
    "    print(f\"üíæ Memory usage: {sys.getsizeof(globals())} bytes\")\n",
    "    \n",
    "    # 3. Check variable state\n",
    "    important_vars = ['MODEL_PATH', 'EXP_NAME', 'model', 'device', 'config']\n",
    "    for var in important_vars:\n",
    "        if var in globals():\n",
    "            print(f\"‚úÖ {var}: Defined\")\n",
    "        else:\n",
    "            print(f\"‚ùå {var}: Not defined\")\n",
    "    \n",
    "    print(\"\\nüéØ Repair suggestions:\")\n",
    "    print(\"1. If cell background is dimmed, double-click the cell to enter edit mode\")\n",
    "    print(\"2. Press Ctrl+Z to undo recent modifications\")\n",
    "    print(\"3. Press Ctrl+Shift+P and search for 'Reload Window'\")\n",
    "    print(\"4. If issues persist, restart kernel (Ctrl+Shift+P -> 'Restart Kernel')\")\n",
    "    \n",
    "    # JavaScript to fix frontend display issues\n",
    "    display(Javascript(\"\"\"\n",
    "    console.log('Fixing cell state...');\n",
    "    \n",
    "    // Remove possible locked state\n",
    "    document.querySelectorAll('.cell').forEach(cell => {\n",
    "        cell.classList.remove('readonly', 'locked', 'disabled');\n",
    "        cell.style.opacity = '1';\n",
    "        cell.style.backgroundColor = '';\n",
    "    });\n",
    "    \n",
    "    // Reactivate edit functionality\n",
    "    document.querySelectorAll('.jp-Cell').forEach(cell => {\n",
    "        cell.style.opacity = '1';\n",
    "        cell.style.filter = 'none';\n",
    "    });\n",
    "    \n",
    "    console.log('Cell state repair completed');\n",
    "    \"\"\"))\n",
    "\n",
    "# ===========================\n",
    "# 3. MAIN TESTING EXECUTION\n",
    "# ===========================\n",
    "\n",
    "def run_comprehensive_cpu_tests():\n",
    "    \"\"\"Run all CPU tests comprehensively\"\"\"\n",
    "    print(\"üöÄ Starting Comprehensive CPU Testing Suite\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Verify required variables exist\n",
    "    try:\n",
    "        print(f\"‚úÖ Using model path: {MODEL_PATH}\")\n",
    "        print(f\"‚úÖ Using experiment config: {EXP_NAME}\")\n",
    "    except NameError:\n",
    "        print(\"‚ùå Please run the model path configuration cell first!\")\n",
    "        return\n",
    "    \n",
    "    # Load configuration\n",
    "    try:\n",
    "        with open(\"config/data_config.yaml\", \"r\") as f:\n",
    "            default_config = yaml.safe_load(f)\n",
    "        config = default_config\n",
    "        \n",
    "        with open(f'config/{EXP_NAME}.yaml', \"r\") as f:\n",
    "            user_config = yaml.safe_load(f)\n",
    "        config.update(user_config)\n",
    "        latent_size = config['image_size'] // 8\n",
    "        \n",
    "        print(f\"‚úÖ Configuration loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Configuration loading failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create mock models\n",
    "    print(\"\\nüèóÔ∏è  Creating mock models for CPU testing...\")\n",
    "    \n",
    "    global model, diffusion, vae, device\n",
    "    \n",
    "    model = MockHybridModel(\n",
    "        latent_size=latent_size,\n",
    "        memory_enabled=config.get('memory_enabled', True),\n",
    "        memory_buffer_size=config.get('memory_buffer_size', 100)\n",
    "    )\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "    model.reset_memory()\n",
    "    \n",
    "    diffusion = MockDiffusion()\n",
    "    vae = MockVAE(device)\n",
    "    \n",
    "    print(\"‚úÖ Mock models created successfully\")\n",
    "    \n",
    "    # Run all tests\n",
    "    test_results = {}\n",
    "    \n",
    "    try:\n",
    "        test_results['file_structure'] = test_file_structure()\n",
    "        test_results['config_loading'] = test_config_loading()\n",
    "        test_results['memory_storage'] = test_memory_storage_mechanism()\n",
    "        test_results['memory_retrieval'] = test_memory_retrieval_criteria()\n",
    "        test_results['integration'] = test_integration()\n",
    "        \n",
    "        # Additional demonstrations\n",
    "        navigation_demo = create_mock_navigation_demo()\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìã COMPREHENSIVE TEST SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"‚úÖ Passed tests:\")\n",
    "        if all(test_results['file_structure'].values()):\n",
    "            print(\"   - File structure validation\")\n",
    "        if all(test_results['config_loading'].values()):\n",
    "            print(\"   - Configuration file loading\")\n",
    "        if test_results['memory_storage']['frames_added'] > 0:\n",
    "            print(\"   - Memory storage mechanism\")\n",
    "        if test_results['memory_retrieval']:\n",
    "            print(\"   - Memory retrieval criteria\")\n",
    "        if test_results['integration']:\n",
    "            print(\"   - System integration\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è  Issues requiring attention:\")\n",
    "        failed_files = [k for k, v in test_results['file_structure'].items() if not v]\n",
    "        if failed_files:\n",
    "            print(f\"   - Missing files/directories: {', '.join(failed_files)}\")\n",
    "        \n",
    "        failed_configs = [k for k, v in test_results['config_loading'].items() if not v]\n",
    "        if failed_configs:\n",
    "            print(f\"   - Configuration loading failed: {', '.join(failed_configs)}\")\n",
    "        \n",
    "        # Repair utilities\n",
    "        fix_notebook_state()\n",
    "        \n",
    "        print(\"\\nüéâ Comprehensive testing completed!\")\n",
    "        print(\"üí° Note: This is a simulation environment, actual GPU environment may perform differently\")\n",
    "        \n",
    "        return test_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Testing failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the comprehensive test suite\n",
    "if __name__ == \"__main__\" or True:  # Allow execution in notebook\n",
    "    test_results = run_comprehensive_cpu_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
